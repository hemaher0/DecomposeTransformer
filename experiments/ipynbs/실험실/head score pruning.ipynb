{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from src.utils.helper import Config, color_print\n",
    "from src.utils.load import load_model, load_data, save_checkpoint, load_checkpoint\n",
    "from src.models.evaluate import evaluate_model, get_sparsity, get_similarity\n",
    "from src.utils.sampling import SamplingDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"bert-4-128-yahoo\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "checkpoint = None\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "num_samples = 128\n",
    "ratio = 0.5\n",
    "seed = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model.\n",
      "{'architectures': 'bert',\n",
      " 'dataset_name': 'YahooAnswersTopics',\n",
      " 'model_name': 'models/bert-4-128-yahoo',\n",
      " 'num_labels': 10,\n",
      " 'tokenizer_name': 'fabriceyhc/bert-base-uncased-yahoo_answers_topics'}\n",
      "The model models/bert-4-128-yahoo is loaded.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset YahooAnswersTopics.\n",
      "train.pkl is loaded from cache.\n",
      "valid.pkl is loaded from cache.\n",
      "test.pkl is loaded from cache.\n",
      "The dataset YahooAnswersTopics is loaded\n",
      "{'config_name': 'yahoo_answers_topics',\n",
      " 'features': {'first_column': 'question_title', 'second_column': 'topic'},\n",
      " 'path': 'yahoo_answers_topics'}\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
    "    config,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    do_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import *\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from transformers.pytorch_utils import (\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_head_importance(\n",
    "    model,\n",
    "    config,\n",
    "    data,\n",
    "    normalize_scores_by_layer=True,\n",
    "):\n",
    "    device = config.device\n",
    "    from functools import partial\n",
    "\n",
    "    gradients = {}\n",
    "    context_layers = {}\n",
    "\n",
    "    def save_grad(gradients, layer_idx, grad):\n",
    "        gradients[f\"context_layer_{layer_idx}\"] = grad\n",
    "\n",
    "    def forward_hook(module, input, output, gradients, context_layers, layer_idx):\n",
    "        context_layers[f\"context_layer_{layer_idx}\"] = output[0]\n",
    "        output[0].register_hook(partial(save_grad, gradients, layer_idx))\n",
    "\n",
    "    def reshape(tensors, shape, num_heads):\n",
    "        batch_size = shape[0]\n",
    "        seq_len = shape[1]\n",
    "        head_dim = shape[2] // num_heads\n",
    "        tensors = tensors.reshape(batch_size, seq_len, num_heads, head_dim)\n",
    "        tensors = tensors.permute(0, 2, 1, 3)\n",
    "        return tensors\n",
    "\n",
    "    forward_handles = []\n",
    "\n",
    "    for layer_idx in range(model.bert.config.num_hidden_layers):\n",
    "        self_att = model.bert.encoder.layer[layer_idx].attention.self\n",
    "        handle = self_att.register_forward_hook(\n",
    "            partial(\n",
    "                forward_hook,\n",
    "                gradients=gradients,\n",
    "                context_layers=context_layers,\n",
    "                layer_idx=layer_idx,\n",
    "            )\n",
    "        )\n",
    "        forward_handles.append(handle)\n",
    "\n",
    "    \"\"\"Calculate head importance scores\"\"\"\n",
    "    # Disable dropout\n",
    "    model.eval()\n",
    "    # Device\n",
    "    device = device or next(model.parameters()).device\n",
    "\n",
    "    # Prepare data loader\n",
    "    # Head importance tensor\n",
    "    n_layers = model.bert.config.num_hidden_layers\n",
    "    n_heads = model.bert.config.num_attention_heads\n",
    "    head_importance = torch.zeros(n_layers, n_heads).to(device)\n",
    "    tot_tokens = 0\n",
    "    first_batch = next(iter(data))\n",
    "    is_embeds = \"embeddings\" in first_batch\n",
    "    for step, batch in enumerate(data):\n",
    "        if is_embeds:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "        else:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        label_ids = batch[\"labels\"].to(device)\n",
    "        # Compute gradients\n",
    "        if is_embeds:\n",
    "            loss = model(\n",
    "                inputs_embeds=embeddings, attention_mask=input_mask, labels=label_ids\n",
    "            ).loss\n",
    "        else:\n",
    "            loss = model(input_ids, attention_mask=input_mask, labels=label_ids).loss\n",
    "        loss.backward()\n",
    "\n",
    "        for layer_idx in range(model.bert.config.num_hidden_layers):\n",
    "            ctx = context_layers[f\"context_layer_{layer_idx}\"]\n",
    "            grad_ctx = gradients[f\"context_layer_{layer_idx}\"]\n",
    "            shape = ctx.shape\n",
    "            ctx = reshape(ctx, shape, n_heads)\n",
    "            grad_ctx = reshape(grad_ctx, shape, n_heads)\n",
    "\n",
    "            # Take the dot\n",
    "            dot = torch.einsum(\"bhli,bhli->bhl\", [grad_ctx, ctx])\n",
    "            head_importance[layer_idx] += dot.abs().sum(-1).sum(0).detach()\n",
    "            del ctx, grad_ctx, dot\n",
    "\n",
    "        tot_tokens += input_mask.float().detach().sum().data\n",
    "\n",
    "    head_importance[:-1] /= tot_tokens\n",
    "\n",
    "    for handle in forward_handles:\n",
    "        handle.remove()\n",
    "    return head_importance\n",
    "\n",
    "def normalize(tensors):\n",
    "    exponent = 2\n",
    "    norm_by_layer = torch.pow(\n",
    "        torch.pow(tensors, exponent).sum(-1), 1 / exponent\n",
    "    )\n",
    "    tensors /= norm_by_layer.unsqueeze(-1) + 1e-20\n",
    "    return tensors\n",
    "\n",
    "def head_importance_prunning(\n",
    "    model, config, dominant_concern, sparsity_ratio, method=\"unstructed\", scheduler=None\n",
    "):\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    num_hidden_layers = model.config.num_hidden_layers\n",
    "    model = model.to(config.device)\n",
    "    total_heads_to_prune = int(num_attention_heads * num_hidden_layers * sparsity_ratio)\n",
    "\n",
    "    total_heads_to_prune = max(total_heads_to_prune, num_hidden_layers)\n",
    "    print(f\"Total heads to prune: {total_heads_to_prune}\")\n",
    "    pruned_heads = set()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        steps = scheduler.get_steps()\n",
    "    else:\n",
    "        steps = [0.25, 0.25, 0.25, 0.25]\n",
    "        # steps = [1.0]\n",
    "\n",
    "    for step_ratio in steps:\n",
    "        heads_to_prune = int(total_heads_to_prune * step_ratio)\n",
    "        \n",
    "        head_importance_list = calculate_head_importance(\n",
    "            model, config, dominant_concern\n",
    "        )\n",
    "        head_importance_list = head_importance_list.cpu()\n",
    "        print(f\"head_importance_list\\n {head_importance_list}\")\n",
    "        head_importance_list = normalize(head_importance_list)\n",
    "        \n",
    "        head_score = head_importance_list\n",
    "        \n",
    "        print(f\"head_score\\n {head_score}\")\n",
    "        if method == \"unstructed\":\n",
    "            sorted_indices = torch.argsort(head_score.view(-1))\n",
    "            prune_list = []\n",
    "            for idx in sorted_indices:\n",
    "                layer_index = int(idx // num_attention_heads)\n",
    "                head_index = int(idx % num_attention_heads)\n",
    "\n",
    "                if (layer_index, head_index) not in pruned_heads:\n",
    "                    prune_list.append((layer_index, head_index))\n",
    "                \n",
    "                if len(prune_list) >= heads_to_prune:\n",
    "                    break\n",
    "            \n",
    "        elif method == \"structed\":\n",
    "            heads_per_layer = heads_to_prune // num_hidden_layers\n",
    "            prune_list = []\n",
    "            for layer_idx in range(num_hidden_layers):\n",
    "                sorted_heads = torch.argsort(head_importance_list[layer_idx])\n",
    "                prune_list.extend(\n",
    "                    [\n",
    "                        (layer_idx, head.item())\n",
    "                        for head in sorted_heads[:heads_per_layer]\n",
    "                    ]\n",
    "                )\n",
    "        for layer_index, head_index in prune_list:\n",
    "            if (layer_index, head_index) not in pruned_heads:\n",
    "                prune_heads(\n",
    "                    model.bert.encoder.layer[layer_index].attention,\n",
    "                    [head_index],\n",
    "                    method=method,\n",
    "                )\n",
    "                pruned_heads.add((layer_index, head_index))\n",
    "        print(sorted(pruned_heads))\n",
    "\n",
    "\n",
    "def prune_heads(layer, heads, method):\n",
    "    if len(heads) == 0:\n",
    "        return\n",
    "    heads, index = find_pruneable_heads_and_indices(\n",
    "        heads,\n",
    "        layer.self.num_attention_heads,\n",
    "        layer.self.attention_head_size,\n",
    "        layer.pruned_heads,\n",
    "    )\n",
    "\n",
    "    # if method == \"unstructed\":\n",
    "    layer.self.query = zero_out_head_weights(\n",
    "        layer.self.query, heads, layer.self.attention_head_size\n",
    "    )\n",
    "    layer.self.key = zero_out_head_weights(\n",
    "        layer.self.key, heads, layer.self.attention_head_size\n",
    "    )\n",
    "    layer.self.value = zero_out_head_weights(\n",
    "        layer.self.value, heads, layer.self.attention_head_size\n",
    "    )\n",
    "    layer.output.dense = zero_out_head_weights(\n",
    "        layer.output.dense, heads, layer.self.attention_head_size, dim=1\n",
    "    )\n",
    "    # elif method == \"structed\":\n",
    "    #     layer.self.query = prune_linear_layer(layer.self.query, index)\n",
    "    #     layer.self.key = prune_linear_layer(layer.self.key, index)\n",
    "    #     layer.self.value = prune_linear_layer(layer.self.value, index)\n",
    "    #     layer.output.dense = prune_linear_layer(layer.output.dense, index)\n",
    "\n",
    "    #     layer.self.num_attention_heads = layer.self.num_attention_heads - len(heads)\n",
    "    #     layer.self.all_head_size = layer.self.attention_head_size *  layer.self.num_attention_heads\n",
    "    #     layer.pruned_heads = layer.pruned_heads.union(heads)\n",
    "\n",
    "\n",
    "def zero_out_head_weights(\n",
    "    layer: nn.Linear, heads: Set[int], head_size: int, dim: int = 0\n",
    ") -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Zero out the weights of the specified heads in the linear layer.\n",
    "\n",
    "    Args:\n",
    "        layer (`torch.nn.Linear`): The layer to modify.\n",
    "        heads (`Set[int]`): The indices of heads to zero out.\n",
    "        head_size (`int`): The size of each head.\n",
    "        dim (`int`, *optional*, defaults to 0): The dimension on which to zero out the weights.\n",
    "\n",
    "    Returns:\n",
    "        `torch.nn.Linear`: The modified layer with weights of specified heads zeroed out.\n",
    "    \"\"\"\n",
    "    for head in heads:\n",
    "        start_index = head * head_size\n",
    "        end_index = (head + 1) * head_size\n",
    "        if dim == 0:\n",
    "            layer.weight.data[start_index:end_index] = 0\n",
    "        elif dim == 1:\n",
    "            layer.weight.data[:, start_index:end_index] = 0\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.evaluate import (\n",
    "    evaluate_model,\n",
    "    get_sparsity,\n",
    "    get_similarity,\n",
    "    get_perplexity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.8039e-03, 2.0566e-03, 1.8249e-03, 1.8589e-03],\n",
      "        [1.5613e-03, 2.5663e-03, 2.0641e-03, 2.2096e-03],\n",
      "        [1.6074e-03, 2.3024e-03, 1.6433e-03, 1.7042e-03],\n",
      "        [1.9489e+00, 1.6367e+00, 2.2909e+00, 1.9624e+00]])\n",
      "head_score\n",
      " tensor([[0.4775, 0.5444, 0.4831, 0.4921],\n",
      "        [0.3663, 0.6021, 0.4843, 0.5184],\n",
      "        [0.4377, 0.6269, 0.4474, 0.4640],\n",
      "        [0.4938, 0.4147, 0.5804, 0.4972]])\n",
      "[(1, 0), (3, 1)]\n",
      "head_importance_list\n",
      " tensor([[1.8696e-03, 1.9767e-03, 1.9059e-03, 1.8763e-03],\n",
      "        [0.0000e+00, 2.6282e-03, 2.0795e-03, 2.2699e-03],\n",
      "        [1.5154e-03, 2.1690e-03, 1.7973e-03, 1.6888e-03],\n",
      "        [2.0914e+00, 0.0000e+00, 2.5977e+00, 2.2976e+00]])\n",
      "head_score\n",
      " tensor([[0.4900, 0.5181, 0.4996, 0.4918],\n",
      "        [0.0000, 0.6493, 0.5137, 0.5608],\n",
      "        [0.4190, 0.5997, 0.4969, 0.4669],\n",
      "        [0.5164, 0.0000, 0.6414, 0.5673]])\n",
      "[(1, 0), (2, 0), (2, 3), (3, 1)]\n",
      "head_importance_list\n",
      " tensor([[1.8762e-03, 2.0669e-03, 1.9711e-03, 1.9926e-03],\n",
      "        [0.0000e+00, 2.7047e-03, 2.0982e-03, 2.2961e-03],\n",
      "        [0.0000e+00, 2.3888e-03, 1.9501e-03, 0.0000e+00],\n",
      "        [2.0797e+00, 0.0000e+00, 2.8067e+00, 2.3114e+00]])\n",
      "head_score\n",
      " tensor([[0.4743, 0.5225, 0.4983, 0.5037],\n",
      "        [0.0000, 0.6562, 0.5090, 0.5570],\n",
      "        [0.0000, 0.7746, 0.6324, 0.0000],\n",
      "        [0.4965, 0.0000, 0.6701, 0.5518]])\n",
      "[(0, 0), (1, 0), (2, 0), (2, 3), (3, 0), (3, 1)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.1368e-03, 2.0217e-03, 2.0908e-03],\n",
      "        [0.0000e+00, 2.7019e-03, 2.2089e-03, 2.3631e-03],\n",
      "        [0.0000e+00, 2.3995e-03, 2.0304e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 3.1458e+00, 2.4747e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.5921, 0.5602, 0.5793],\n",
      "        [0.0000, 0.6411, 0.5241, 0.5607],\n",
      "        [0.0000, 0.7634, 0.6460, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7860, 0.6183]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 2), (2, 0), (2, 3), (3, 0), (3, 1)]\n",
      "Evaluate the pruned model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4768a68bba39497d86e0902ea9b6d7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3344\n",
      "Precision: 0.6432, Recall: 0.5870, F1-Score: 0.5965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4406    0.5973    0.5071      2992\n",
      "           1     0.6312    0.5010    0.5586      2992\n",
      "           2     0.6487    0.6315    0.6400      3012\n",
      "           3     0.3183    0.6448    0.4262      2998\n",
      "           4     0.7624    0.7101    0.7353      2973\n",
      "           5     0.8481    0.6637    0.7447      3054\n",
      "           6     0.7567    0.3367    0.4660      3003\n",
      "           7     0.5724    0.6232    0.5967      3012\n",
      "           8     0.6743    0.5761    0.6213      2982\n",
      "           9     0.7789    0.5858    0.6687      2982\n",
      "\n",
      "    accuracy                         0.5871     30000\n",
      "   macro avg     0.6432    0.5870    0.5965     30000\n",
      "weighted avg     0.6434    0.5871    0.5966     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9844533734504439\n",
      "CCA coefficients mean non-concern: 0.9809838096902131\n",
      "Linear CKA concern: 0.9416096580710461\n",
      "Linear CKA non-concern: 0.9520544497291183\n",
      "Kernel CKA concern: 0.8889924674766354\n",
      "Kernel CKA non-concern: 0.9028396749666748\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.637533187866211\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.6000e-03, 2.4334e-03, 1.5432e-03, 1.9861e-03],\n",
      "        [1.0973e-03, 2.2050e-03, 1.5707e-03, 1.6602e-03],\n",
      "        [1.3612e-03, 1.7103e-03, 1.4790e-03, 1.1665e-03],\n",
      "        [1.2109e+00, 2.2736e+00, 1.9573e+00, 2.2728e+00]])\n",
      "head_score\n",
      " tensor([[0.4158, 0.6324, 0.4010, 0.5161],\n",
      "        [0.3266, 0.6563, 0.4675, 0.4941],\n",
      "        [0.4717, 0.5927, 0.5126, 0.4043],\n",
      "        [0.3063, 0.5750, 0.4951, 0.5748]])\n",
      "[(1, 0), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[1.6451e-03, 2.4225e-03, 1.5783e-03, 2.0469e-03],\n",
      "        [0.0000e+00, 2.2113e-03, 1.6123e-03, 1.7425e-03],\n",
      "        [1.2668e-03, 1.6460e-03, 1.5697e-03, 1.1390e-03],\n",
      "        [0.0000e+00, 2.3836e+00, 2.1062e+00, 2.6193e+00]])\n",
      "head_score\n",
      " tensor([[0.4212, 0.6202, 0.4041, 0.5241],\n",
      "        [0.0000, 0.6816, 0.4970, 0.5371],\n",
      "        [0.4458, 0.5792, 0.5524, 0.4008],\n",
      "        [0.0000, 0.5785, 0.5112, 0.6357]])\n",
      "[(0, 2), (1, 0), (2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[1.7015e-03, 2.5617e-03, 0.0000e+00, 2.1579e-03],\n",
      "        [0.0000e+00, 2.3159e-03, 1.6177e-03, 1.7872e-03],\n",
      "        [1.3582e-03, 1.7744e-03, 1.6916e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4001e+00, 2.2364e+00, 2.6838e+00]])\n",
      "head_score\n",
      " tensor([[0.4529, 0.6819, 0.0000, 0.5744],\n",
      "        [0.0000, 0.6928, 0.4839, 0.5346],\n",
      "        [0.4846, 0.6331, 0.6036, 0.0000],\n",
      "        [0.0000, 0.5663, 0.5276, 0.6332]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 2), (2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 3.1320e-03, 0.0000e+00, 2.5053e-03],\n",
      "        [0.0000e+00, 2.5860e-03, 0.0000e+00, 1.9819e-03],\n",
      "        [1.5178e-03, 1.8346e-03, 1.9640e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.7794e+00, 2.4574e+00, 3.1407e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.7809, 0.0000, 0.6246],\n",
      "        [0.0000, 0.7937, 0.0000, 0.6083],\n",
      "        [0.4917, 0.5944, 0.6363, 0.0000],\n",
      "        [0.0000, 0.5718, 0.5056, 0.6461]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 2), (2, 0), (2, 3), (3, 0), (3, 2)]\n",
      "Evaluate the pruned model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2427c2895c048c1a05c86e3c57d64f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3359\n",
      "Precision: 0.6407, Recall: 0.5867, F1-Score: 0.5960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4324    0.6080    0.5053      2992\n",
      "           1     0.6342    0.4873    0.5511      2992\n",
      "           2     0.6404    0.6345    0.6374      3012\n",
      "           3     0.3243    0.6334    0.4290      2998\n",
      "           4     0.7681    0.6862    0.7248      2973\n",
      "           5     0.8774    0.6516    0.7478      3054\n",
      "           6     0.7505    0.3457    0.4733      3003\n",
      "           7     0.5737    0.6139    0.5931      3012\n",
      "           8     0.6577    0.5812    0.6171      2982\n",
      "           9     0.7486    0.6251    0.6813      2982\n",
      "\n",
      "    accuracy                         0.5867     30000\n",
      "   macro avg     0.6407    0.5867    0.5960     30000\n",
      "weighted avg     0.6410    0.5867    0.5962     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9864824868272253\n",
      "CCA coefficients mean non-concern: 0.9800235211038035\n",
      "Linear CKA concern: 0.9276034167660696\n",
      "Linear CKA non-concern: 0.9406325935507258\n",
      "Kernel CKA concern: 0.8743406619175325\n",
      "Kernel CKA non-concern: 0.8910210812370539\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.634608268737793\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.3089e-03, 1.7894e-03, 1.6850e-03, 2.1818e-03],\n",
      "        [1.4560e-03, 4.0453e-03, 2.0574e-03, 1.7450e-03],\n",
      "        [1.6540e-03, 1.8103e-03, 1.7322e-03, 1.3696e-03],\n",
      "        [2.0138e+00, 1.5445e+00, 1.9408e+00, 2.5295e+00]])\n",
      "head_score\n",
      " tensor([[0.3700, 0.5058, 0.4763, 0.6168],\n",
      "        [0.2869, 0.7970, 0.4053, 0.3438],\n",
      "        [0.5012, 0.5486, 0.5249, 0.4150],\n",
      "        [0.4942, 0.3790, 0.4763, 0.6207]])\n",
      "[(1, 0), (1, 3)]\n",
      "head_importance_list\n",
      " tensor([[1.3003e-03, 1.8147e-03, 1.7362e-03, 2.2207e-03],\n",
      "        [0.0000e+00, 4.1709e-03, 2.0310e-03, 0.0000e+00],\n",
      "        [1.5979e-03, 1.8441e-03, 1.9773e-03, 1.4283e-03],\n",
      "        [2.1017e+00, 1.5589e+00, 1.9996e+00, 2.5166e+00]])\n",
      "head_score\n",
      " tensor([[0.3616, 0.5047, 0.4828, 0.6176],\n",
      "        [0.0000, 0.8991, 0.4378, 0.0000],\n",
      "        [0.4631, 0.5345, 0.5731, 0.4140],\n",
      "        [0.5071, 0.3761, 0.4824, 0.6072]])\n",
      "[(0, 0), (1, 0), (1, 3), (3, 1)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.0408e-03, 1.9674e-03, 2.4638e-03],\n",
      "        [0.0000e+00, 4.3841e-03, 2.2377e-03, 0.0000e+00],\n",
      "        [1.6594e-03, 1.9263e-03, 2.1378e-03, 1.5978e-03],\n",
      "        [2.3684e+00, 0.0000e+00, 2.2168e+00, 2.8857e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.5434, 0.5238, 0.6560],\n",
      "        [0.0000, 0.8907, 0.4546, 0.0000],\n",
      "        [0.4502, 0.5226, 0.5800, 0.4335],\n",
      "        [0.5455, 0.0000, 0.5106, 0.6646]])\n",
      "[(0, 0), (1, 0), (1, 3), (2, 0), (2, 3), (3, 1)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.2412e-03, 2.0845e-03, 2.5410e-03],\n",
      "        [0.0000e+00, 4.3752e-03, 2.2477e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0279e-03, 2.2725e-03, 0.0000e+00],\n",
      "        [2.6597e+00, 0.0000e+00, 2.4277e+00, 3.0848e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.5634, 0.5240, 0.6387],\n",
      "        [0.0000, 0.8895, 0.4570, 0.0000],\n",
      "        [0.0000, 0.6658, 0.7461, 0.0000],\n",
      "        [0.5609, 0.0000, 0.5120, 0.6506]])\n",
      "[(0, 0), (1, 0), (1, 2), (1, 3), (2, 0), (2, 3), (3, 1), (3, 2)]\n",
      "Evaluate the pruned model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba45945f1e98496e8fcdbd2c7c41294d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3256\n",
      "Precision: 0.6312, Recall: 0.5844, F1-Score: 0.5907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4046    0.6213    0.4900      2992\n",
      "           1     0.6169    0.5582    0.5861      2992\n",
      "           2     0.6421    0.6451    0.6436      3012\n",
      "           3     0.3416    0.5807    0.4301      2998\n",
      "           4     0.6661    0.8032    0.7283      2973\n",
      "           5     0.8263    0.6372    0.7195      3054\n",
      "           6     0.7132    0.3593    0.4779      3003\n",
      "           7     0.6174    0.5664    0.5908      3012\n",
      "           8     0.6751    0.5540    0.6086      2982\n",
      "           9     0.8091    0.5188    0.6322      2982\n",
      "\n",
      "    accuracy                         0.5844     30000\n",
      "   macro avg     0.6312    0.5844    0.5907     30000\n",
      "weighted avg     0.6315    0.5844    0.5908     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9841754115418812\n",
      "CCA coefficients mean non-concern: 0.9785827868540806\n",
      "Linear CKA concern: 0.91201322989436\n",
      "Linear CKA non-concern: 0.9172036398442881\n",
      "Kernel CKA concern: 0.8546250211722984\n",
      "Kernel CKA non-concern: 0.8611269602521653\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.592667818069458\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.2754e-03, 1.6811e-03, 1.2118e-03, 1.3914e-03],\n",
      "        [1.3562e-03, 1.6372e-03, 1.3886e-03, 1.1902e-03],\n",
      "        [1.2916e-03, 2.3353e-03, 1.6129e-03, 1.2222e-03],\n",
      "        [1.3527e+00, 1.8282e+00, 1.5371e+00, 1.6215e+00]])\n",
      "head_score\n",
      " tensor([[0.4550, 0.5997, 0.4323, 0.4964],\n",
      "        [0.4836, 0.5838, 0.4952, 0.4244],\n",
      "        [0.3856, 0.6973, 0.4816, 0.3649],\n",
      "        [0.4243, 0.5734, 0.4821, 0.5086]])\n",
      "[(2, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[1.2360e-03, 1.7549e-03, 1.2284e-03, 1.4427e-03],\n",
      "        [1.4580e-03, 1.6464e-03, 1.4289e-03, 1.2114e-03],\n",
      "        [0.0000e+00, 2.5323e-03, 1.7548e-03, 0.0000e+00],\n",
      "        [1.5135e+00, 2.2622e+00, 1.7998e+00, 1.8278e+00]])\n",
      "head_score\n",
      " tensor([[0.4317, 0.6129, 0.4290, 0.5039],\n",
      "        [0.5047, 0.5699, 0.4946, 0.4193],\n",
      "        [0.0000, 0.8219, 0.5696, 0.0000],\n",
      "        [0.4047, 0.6048, 0.4812, 0.4887]])\n",
      "[(1, 3), (2, 0), (2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[1.2528e-03, 1.8844e-03, 1.2868e-03, 1.5593e-03],\n",
      "        [1.5900e-03, 1.6910e-03, 1.6010e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4594e-03, 1.7475e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.6550e+00, 2.1308e+00, 2.0757e+00]])\n",
      "head_score\n",
      " tensor([[0.4129, 0.6210, 0.4241, 0.5139],\n",
      "        [0.5639, 0.5997, 0.5678, 0.0000],\n",
      "        [0.0000, 0.8152, 0.5792, 0.0000],\n",
      "        [0.0000, 0.6659, 0.5344, 0.5206]])\n",
      "[(0, 0), (0, 2), (1, 3), (2, 0), (2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.1683e-03, 0.0000e+00, 1.7543e-03],\n",
      "        [1.6385e-03, 1.7819e-03, 1.6769e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4496e-03, 1.8584e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.7516e+00, 2.3818e+00, 2.0255e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.7774, 0.0000, 0.6290],\n",
      "        [0.5564, 0.6051, 0.5694, 0.0000],\n",
      "        [0.0000, 0.7967, 0.6044, 0.0000],\n",
      "        [0.0000, 0.6607, 0.5719, 0.4863]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 3), (2, 0), (2, 3), (3, 0), (3, 3)]\n",
      "Evaluate the pruned model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f7df4274c44a118686442cb7518881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3244\n",
      "Precision: 0.6409, Recall: 0.5880, F1-Score: 0.5978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4304    0.6046    0.5028      2992\n",
      "           1     0.7164    0.4111    0.5224      2992\n",
      "           2     0.6439    0.6112    0.6272      3012\n",
      "           3     0.3205    0.6468    0.4286      2998\n",
      "           4     0.7935    0.6953    0.7411      2973\n",
      "           5     0.8300    0.7161    0.7689      3054\n",
      "           6     0.6771    0.3953    0.4992      3003\n",
      "           7     0.5833    0.6116    0.5971      3012\n",
      "           8     0.6558    0.5624    0.6055      2982\n",
      "           9     0.7576    0.6258    0.6854      2982\n",
      "\n",
      "    accuracy                         0.5882     30000\n",
      "   macro avg     0.6409    0.5880    0.5978     30000\n",
      "weighted avg     0.6410    0.5882    0.5980     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9865694594122858\n",
      "CCA coefficients mean non-concern: 0.9809432903678593\n",
      "Linear CKA concern: 0.931608298441058\n",
      "Linear CKA non-concern: 0.9405200861276097\n",
      "Kernel CKA concern: 0.8692708399752558\n",
      "Kernel CKA non-concern: 0.8874464260199603\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.5983669757843018\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.0851e-03, 1.6123e-03, 2.1509e-03, 1.8952e-03],\n",
      "        [1.0772e-03, 2.0314e-03, 1.4582e-03, 1.0514e-03],\n",
      "        [8.4126e-04, 1.6059e-03, 1.2189e-03, 1.2636e-03],\n",
      "        [1.8896e+00, 1.3001e+00, 1.2848e+00, 1.8556e+00]])\n",
      "head_score\n",
      " tensor([[0.3133, 0.4655, 0.6210, 0.5472],\n",
      "        [0.3691, 0.6960, 0.4996, 0.3602],\n",
      "        [0.3333, 0.6363, 0.4830, 0.5007],\n",
      "        [0.5872, 0.4040, 0.3993, 0.5767]])\n",
      "[(0, 0), (2, 0)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 1.6460e-03, 2.1527e-03, 1.9720e-03],\n",
      "        [1.0684e-03, 2.0535e-03, 1.4809e-03, 1.0535e-03],\n",
      "        [0.0000e+00, 1.7222e-03, 1.2397e-03, 1.3956e-03],\n",
      "        [2.1356e+00, 1.4143e+00, 1.2403e+00, 1.9244e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.4911, 0.6423, 0.5884],\n",
      "        [0.3630, 0.6978, 0.5032, 0.3580],\n",
      "        [0.0000, 0.6781, 0.4881, 0.5495],\n",
      "        [0.6216, 0.4117, 0.3610, 0.5601]])\n",
      "[(0, 0), (1, 3), (2, 0), (3, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 1.6951e-03, 2.1831e-03, 1.9679e-03],\n",
      "        [1.0802e-03, 1.9907e-03, 1.4725e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 1.8342e-03, 1.2614e-03, 1.4724e-03],\n",
      "        [2.4318e+00, 1.5834e+00, 0.0000e+00, 2.1010e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.4996, 0.6434, 0.5800],\n",
      "        [0.3999, 0.7369, 0.5451, 0.0000],\n",
      "        [0.0000, 0.6872, 0.4726, 0.5517],\n",
      "        [0.6788, 0.4420, 0.0000, 0.5864]])\n",
      "[(0, 0), (1, 0), (1, 3), (2, 0), (3, 1), (3, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 1.6946e-03, 2.1388e-03, 1.8796e-03],\n",
      "        [0.0000e+00, 1.9969e-03, 1.4689e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 1.8423e-03, 1.3204e-03, 1.6010e-03],\n",
      "        [2.5175e+00, 0.0000e+00, 0.0000e+00, 2.2109e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.5114, 0.6455, 0.5673],\n",
      "        [0.0000, 0.8055, 0.5925, 0.0000],\n",
      "        [0.0000, 0.6639, 0.4758, 0.5769],\n",
      "        [0.7514, 0.0000, 0.0000, 0.6599]])\n",
      "[(0, 0), (0, 1), (1, 0), (1, 3), (2, 0), (2, 2), (3, 1), (3, 2)]\n",
      "Evaluate the pruned model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cc9e58fe9a499caf3f6606754df032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3003\n",
      "Precision: 0.6353, Recall: 0.5969, F1-Score: 0.6032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4197    0.5966    0.4928      2992\n",
      "           1     0.6928    0.4689    0.5593      2992\n",
      "           2     0.6642    0.6325    0.6480      3012\n",
      "           3     0.3421    0.5720    0.4282      2998\n",
      "           4     0.6790    0.7999    0.7345      2973\n",
      "           5     0.8462    0.7299    0.7838      3054\n",
      "           6     0.6645    0.4003    0.4996      3003\n",
      "           7     0.6261    0.6042    0.6150      3012\n",
      "           8     0.6295    0.6107    0.6199      2982\n",
      "           9     0.7886    0.5543    0.6510      2982\n",
      "\n",
      "    accuracy                         0.5970     30000\n",
      "   macro avg     0.6353    0.5969    0.6032     30000\n",
      "weighted avg     0.6356    0.5970    0.6034     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9813158725343135\n",
      "CCA coefficients mean non-concern: 0.9799179974481644\n",
      "Linear CKA concern: 0.9365709084074428\n",
      "Linear CKA non-concern: 0.9416390439053216\n",
      "Kernel CKA concern: 0.9054639157411615\n",
      "Kernel CKA non-concern: 0.8844784926565676\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.5129287242889404\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.2765e-03, 1.8084e-03, 1.2634e-03, 1.5922e-03],\n",
      "        [1.7024e-03, 1.4033e-03, 1.6171e-03, 1.3557e-03],\n",
      "        [1.1430e-03, 9.1175e-04, 7.9373e-04, 1.5471e-03],\n",
      "        [1.3691e+00, 1.2985e+00, 1.6511e+00, 1.4942e+00]])\n",
      "head_score\n",
      " tensor([[0.4248, 0.6018, 0.4204, 0.5298],\n",
      "        [0.5576, 0.4597, 0.5297, 0.4441],\n",
      "        [0.5031, 0.4013, 0.3494, 0.6810],\n",
      "        [0.4691, 0.4449, 0.5657, 0.5119]])\n",
      "[(2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[1.2953e-03, 1.8488e-03, 1.2759e-03, 1.6370e-03],\n",
      "        [1.7339e-03, 1.3169e-03, 1.6759e-03, 1.4231e-03],\n",
      "        [1.3017e-03, 0.0000e+00, 0.0000e+00, 1.6900e-03],\n",
      "        [1.5758e+00, 1.3931e+00, 1.7439e+00, 1.4866e+00]])\n",
      "head_score\n",
      " tensor([[0.4224, 0.6029, 0.4161, 0.5338],\n",
      "        [0.5603, 0.4256, 0.5416, 0.4599],\n",
      "        [0.6102, 0.0000, 0.0000, 0.7922],\n",
      "        [0.5066, 0.4479, 0.5606, 0.4779]])\n",
      "[(0, 0), (0, 2), (2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.1546e-03, 0.0000e+00, 1.9725e-03],\n",
      "        [1.8975e-03, 1.3820e-03, 1.9461e-03, 1.6143e-03],\n",
      "        [1.4174e-03, 0.0000e+00, 0.0000e+00, 1.8893e-03],\n",
      "        [1.8126e+00, 1.5258e+00, 1.8961e+00, 1.6067e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.7376, 0.0000, 0.6753],\n",
      "        [0.5500, 0.4006, 0.5641, 0.4679],\n",
      "        [0.6001, 0.0000, 0.0000, 0.7999],\n",
      "        [0.5279, 0.4444, 0.5522, 0.4679]])\n",
      "[(0, 0), (0, 2), (1, 1), (2, 1), (2, 2), (3, 1)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.0467e-03, 0.0000e+00, 1.9576e-03],\n",
      "        [1.9118e-03, 0.0000e+00, 1.9907e-03, 1.6539e-03],\n",
      "        [1.3546e-03, 0.0000e+00, 0.0000e+00, 1.8716e-03],\n",
      "        [1.9786e+00, 0.0000e+00, 1.8557e+00, 1.6220e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.7227, 0.0000, 0.6912],\n",
      "        [0.5942, 0.0000, 0.6187, 0.5140],\n",
      "        [0.5863, 0.0000, 0.0000, 0.8101],\n",
      "        [0.6260, 0.0000, 0.5871, 0.5132]])\n",
      "[(0, 0), (0, 2), (1, 1), (1, 3), (2, 1), (2, 2), (3, 1), (3, 3)]\n",
      "Evaluate the pruned model 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29694befa8341ee810b86264b1158f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3611\n",
      "Precision: 0.6295, Recall: 0.5738, F1-Score: 0.5813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3640    0.5876    0.4495      2992\n",
      "           1     0.7033    0.4278    0.5320      2992\n",
      "           2     0.7187    0.5259    0.6074      3012\n",
      "           3     0.3385    0.6024    0.4335      2998\n",
      "           4     0.7828    0.6872    0.7319      2973\n",
      "           5     0.7912    0.7705    0.7807      3054\n",
      "           6     0.6884    0.3826    0.4919      3003\n",
      "           7     0.5158    0.6793    0.5863      3012\n",
      "           8     0.6870    0.4195    0.5209      2982\n",
      "           9     0.7054    0.6553    0.6794      2982\n",
      "\n",
      "    accuracy                         0.5741     30000\n",
      "   macro avg     0.6295    0.5738    0.5813     30000\n",
      "weighted avg     0.6296    0.5741    0.5816     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9796360542659446\n",
      "CCA coefficients mean non-concern: 0.9783964626283436\n",
      "Linear CKA concern: 0.9055711864218803\n",
      "Linear CKA non-concern: 0.8905857973318139\n",
      "Kernel CKA concern: 0.8639952154750341\n",
      "Kernel CKA non-concern: 0.8292643794167887\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.7403922080993652\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[2.2948e-03, 2.0340e-03, 2.2162e-03, 2.2799e-03],\n",
      "        [1.6618e-03, 2.8749e-03, 2.4298e-03, 3.0945e-03],\n",
      "        [1.9207e-03, 2.7790e-03, 1.7472e-03, 2.1926e-03],\n",
      "        [2.9985e+00, 4.2756e+00, 2.1452e+00, 3.2290e+00]])\n",
      "head_score\n",
      " tensor([[0.5195, 0.4605, 0.5017, 0.5161],\n",
      "        [0.3228, 0.5584, 0.4719, 0.6011],\n",
      "        [0.4375, 0.6330, 0.3980, 0.4995],\n",
      "        [0.4610, 0.6574, 0.3298, 0.4965]])\n",
      "[(1, 0), (3, 2)]\n",
      "head_importance_list\n",
      " tensor([[2.1579e-03, 2.0393e-03, 2.2978e-03, 2.2629e-03],\n",
      "        [0.0000e+00, 2.8144e-03, 2.3631e-03, 3.2627e-03],\n",
      "        [1.8455e-03, 2.6548e-03, 1.6600e-03, 2.2895e-03],\n",
      "        [3.5490e+00, 4.9618e+00, 0.0000e+00, 3.5927e+00]])\n",
      "head_score\n",
      " tensor([[0.4923, 0.4652, 0.5242, 0.5162],\n",
      "        [0.0000, 0.5727, 0.4809, 0.6639],\n",
      "        [0.4296, 0.6180, 0.3865, 0.5330],\n",
      "        [0.5013, 0.7008, 0.0000, 0.5075]])\n",
      "[(1, 0), (2, 0), (2, 2), (3, 2)]\n",
      "head_importance_list\n",
      " tensor([[2.0180e-03, 2.0397e-03, 2.2747e-03, 2.1922e-03],\n",
      "        [0.0000e+00, 2.7720e-03, 2.3761e-03, 3.3495e-03],\n",
      "        [0.0000e+00, 2.8972e-03, 0.0000e+00, 2.4693e-03],\n",
      "        [3.8367e+00, 5.1242e+00, 0.0000e+00, 3.4454e+00]])\n",
      "head_score\n",
      " tensor([[0.4729, 0.4779, 0.5330, 0.5137],\n",
      "        [0.0000, 0.5595, 0.4796, 0.6760],\n",
      "        [0.0000, 0.7611, 0.0000, 0.6487],\n",
      "        [0.5278, 0.7049, 0.0000, 0.4739]])\n",
      "[(0, 0), (1, 0), (2, 0), (2, 2), (3, 2), (3, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 2.2524e-03, 2.5024e-03, 2.4753e-03],\n",
      "        [0.0000e+00, 2.9166e-03, 2.5156e-03, 3.5384e-03],\n",
      "        [0.0000e+00, 2.9678e-03, 0.0000e+00, 2.6594e-03],\n",
      "        [4.3155e+00, 6.3501e+00, 0.0000e+00, 0.0000e+00]])\n",
      "head_score\n",
      " tensor([[0.0000, 0.5390, 0.5988, 0.5923],\n",
      "        [0.0000, 0.5576, 0.4810, 0.6765],\n",
      "        [0.0000, 0.7447, 0.0000, 0.6674],\n",
      "        [0.5621, 0.8271, 0.0000, 0.0000]])\n",
      "[(0, 0), (0, 1), (1, 0), (1, 2), (2, 0), (2, 2), (3, 2), (3, 3)]\n",
      "Evaluate the pruned model 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a72f9f32844cd098356b13469963dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3141\n",
      "Precision: 0.6451, Recall: 0.5912, F1-Score: 0.5998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3849    0.6324    0.4785      2992\n",
      "           1     0.7682    0.3700    0.4994      2992\n",
      "           2     0.6625    0.6132    0.6369      3012\n",
      "           3     0.3415    0.5881    0.4321      2998\n",
      "           4     0.7396    0.7508    0.7451      2973\n",
      "           5     0.8946    0.6893    0.7786      3054\n",
      "           6     0.6396    0.4279    0.5128      3003\n",
      "           7     0.6299    0.6046    0.6170      3012\n",
      "           8     0.6072    0.6496    0.6277      2982\n",
      "           9     0.7827    0.5858    0.6701      2982\n",
      "\n",
      "    accuracy                         0.5912     30000\n",
      "   macro avg     0.6451    0.5912    0.5998     30000\n",
      "weighted avg     0.6454    0.5912    0.6000     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9827139718291963\n",
      "CCA coefficients mean non-concern: 0.9792353495957742\n",
      "Linear CKA concern: 0.9340463921035543\n",
      "Linear CKA non-concern: 0.9317991050206249\n",
      "Kernel CKA concern: 0.8822502565726451\n",
      "Kernel CKA non-concern: 0.8853560942458312\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.5505387783050537\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.4831e-03, 1.7416e-03, 1.4530e-03, 1.6323e-03],\n",
      "        [1.3250e-03, 1.6977e-03, 1.5724e-03, 1.5391e-03],\n",
      "        [1.4363e-03, 1.3584e-03, 1.1863e-03, 1.4932e-03],\n",
      "        [1.6379e+00, 1.5092e+00, 2.1767e+00, 1.3949e+00]])\n",
      "head_score\n",
      " tensor([[0.4688, 0.5505, 0.4593, 0.5160],\n",
      "        [0.4304, 0.5514, 0.5107, 0.4999],\n",
      "        [0.5229, 0.4945, 0.4319, 0.5436],\n",
      "        [0.4800, 0.4423, 0.6379, 0.4088]])\n",
      "[(1, 0), (3, 3)]\n",
      "head_importance_list\n",
      " tensor([[1.5650e-03, 1.8147e-03, 1.5245e-03, 1.7170e-03],\n",
      "        [0.0000e+00, 1.8463e-03, 1.7630e-03, 1.7168e-03],\n",
      "        [1.4277e-03, 1.3799e-03, 1.2212e-03, 1.6996e-03],\n",
      "        [1.9448e+00, 1.8103e+00, 2.6510e+00, 0.0000e+00]])\n",
      "head_score\n",
      " tensor([[0.4716, 0.5468, 0.4593, 0.5174],\n",
      "        [0.0000, 0.6001, 0.5731, 0.5580],\n",
      "        [0.4949, 0.4783, 0.4233, 0.5891],\n",
      "        [0.5182, 0.4823, 0.7063, 0.0000]])\n",
      "[(0, 2), (1, 0), (2, 2), (3, 3)]\n",
      "head_importance_list\n",
      " tensor([[1.5380e-03, 1.8034e-03, 0.0000e+00, 1.6476e-03],\n",
      "        [0.0000e+00, 1.7598e-03, 1.6821e-03, 1.6406e-03],\n",
      "        [1.3620e-03, 1.3885e-03, 0.0000e+00, 1.7211e-03],\n",
      "        [1.9941e+00, 1.5545e+00, 2.3718e+00, 0.0000e+00]])\n",
      "head_score\n",
      " tensor([[0.5328, 0.6247, 0.0000, 0.5708],\n",
      "        [0.0000, 0.5995, 0.5730, 0.5589],\n",
      "        [0.5244, 0.5346, 0.0000, 0.6627],\n",
      "        [0.5752, 0.4484, 0.6842, 0.0000]])\n",
      "[(0, 2), (1, 0), (2, 0), (2, 2), (3, 1), (3, 3)]\n",
      "head_importance_list\n",
      " tensor([[1.6725e-03, 1.9525e-03, 0.0000e+00, 1.8059e-03],\n",
      "        [0.0000e+00, 1.7801e-03, 1.8119e-03, 1.7524e-03],\n",
      "        [0.0000e+00, 1.4731e-03, 0.0000e+00, 2.0682e-03],\n",
      "        [2.5389e+00, 0.0000e+00, 3.0617e+00, 0.0000e+00]])\n",
      "head_score\n",
      " tensor([[0.5324, 0.6214, 0.0000, 0.5748],\n",
      "        [0.0000, 0.5769, 0.5872, 0.5679],\n",
      "        [0.0000, 0.5802, 0.0000, 0.8145],\n",
      "        [0.6383, 0.0000, 0.7698, 0.0000]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 3), (2, 0), (2, 2), (3, 1), (3, 3)]\n",
      "Evaluate the pruned model 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b4f792aeea4887ad14469417ba8185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3033\n",
      "Precision: 0.6448, Recall: 0.5904, F1-Score: 0.6007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4109    0.6003    0.4878      2992\n",
      "           1     0.7222    0.4215    0.5323      2992\n",
      "           2     0.6790    0.6046    0.6396      3012\n",
      "           3     0.3190    0.6291    0.4233      2998\n",
      "           4     0.7577    0.7511    0.7544      2973\n",
      "           5     0.8448    0.7397    0.7888      3054\n",
      "           6     0.6393    0.4179    0.5054      3003\n",
      "           7     0.6103    0.6394    0.6245      3012\n",
      "           8     0.6568    0.5731    0.6121      2982\n",
      "           9     0.8083    0.5275    0.6384      2982\n",
      "\n",
      "    accuracy                         0.5906     30000\n",
      "   macro avg     0.6448    0.5904    0.6007     30000\n",
      "weighted avg     0.6450    0.5906    0.6009     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9842812760869847\n",
      "CCA coefficients mean non-concern: 0.9819642570982506\n",
      "Linear CKA concern: 0.9389228718366296\n",
      "Linear CKA non-concern: 0.9361525531532319\n",
      "Kernel CKA concern: 0.8881937452474895\n",
      "Kernel CKA non-concern: 0.8874866265972624\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.5352988243103027\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[2.0747e-03, 1.0198e-03, 1.3625e-03, 1.4109e-03],\n",
      "        [7.4922e-04, 2.0140e-03, 1.2541e-03, 1.3431e-03],\n",
      "        [9.3180e-04, 9.8292e-04, 9.7096e-04, 9.7983e-04],\n",
      "        [1.9126e+00, 1.7218e+00, 1.2340e+00, 1.3557e+00]])\n",
      "head_score\n",
      " tensor([[0.6843, 0.3364, 0.4494, 0.4654],\n",
      "        [0.2650, 0.7123, 0.4436, 0.4750],\n",
      "        [0.4820, 0.5084, 0.5023, 0.5068],\n",
      "        [0.6053, 0.5449, 0.3906, 0.4291]])\n",
      "[(0, 1), (1, 0)]\n",
      "head_importance_list\n",
      " tensor([[2.1468e-03, 0.0000e+00, 1.3756e-03, 1.4839e-03],\n",
      "        [0.0000e+00, 2.0716e-03, 1.3042e-03, 1.3680e-03],\n",
      "        [9.7548e-04, 9.8032e-04, 1.0814e-03, 1.0267e-03],\n",
      "        [1.7647e+00, 1.6449e+00, 1.1965e+00, 1.3917e+00]])\n",
      "head_score\n",
      " tensor([[0.7277, 0.0000, 0.4663, 0.5030],\n",
      "        [0.0000, 0.7387, 0.4651, 0.4878],\n",
      "        [0.4797, 0.4820, 0.5317, 0.5048],\n",
      "        [0.5822, 0.5427, 0.3947, 0.4591]])\n",
      "[(0, 1), (1, 0), (3, 2), (3, 3)]\n",
      "head_importance_list\n",
      " tensor([[2.0639e-03, 0.0000e+00, 1.5185e-03, 1.4976e-03],\n",
      "        [0.0000e+00, 2.2354e-03, 1.2641e-03, 1.3820e-03],\n",
      "        [9.3769e-04, 9.5318e-04, 9.7420e-04, 9.8281e-04],\n",
      "        [2.2720e+00, 2.4578e+00, 0.0000e+00, 0.0000e+00]])\n",
      "head_score\n",
      " tensor([[0.6954, 0.0000, 0.5116, 0.5046],\n",
      "        [0.0000, 0.7665, 0.4335, 0.4739],\n",
      "        [0.4873, 0.4953, 0.5063, 0.5107],\n",
      "        [0.6788, 0.7343, 0.0000, 0.0000]])\n",
      "[(0, 1), (1, 0), (1, 2), (1, 3), (3, 2), (3, 3)]\n",
      "head_importance_list\n",
      " tensor([[2.5987e-03, 0.0000e+00, 1.9510e-03, 1.8564e-03],\n",
      "        [0.0000e+00, 3.0861e-03, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1231e-03, 1.2683e-03, 1.3419e-03, 1.2131e-03],\n",
      "        [2.3634e+00, 2.4373e+00, 0.0000e+00, 0.0000e+00]])\n",
      "head_score\n",
      " tensor([[0.6944, 0.0000, 0.5213, 0.4960],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.4532, 0.5118, 0.5414, 0.4895],\n",
      "        [0.6961, 0.7179, 0.0000, 0.0000]])\n",
      "[(0, 1), (1, 0), (1, 2), (1, 3), (2, 0), (2, 3), (3, 2), (3, 3)]\n",
      "Evaluate the pruned model 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0690cc96521d440b986678daf5d41b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3298\n",
      "Precision: 0.6324, Recall: 0.5890, F1-Score: 0.5963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3853    0.6404    0.4811      2992\n",
      "           1     0.6970    0.4666    0.5590      2992\n",
      "           2     0.6342    0.6408    0.6375      3012\n",
      "           3     0.3633    0.5720    0.4444      2998\n",
      "           4     0.7273    0.7588    0.7427      2973\n",
      "           5     0.8520    0.6012    0.7049      3054\n",
      "           6     0.6638    0.4089    0.5061      3003\n",
      "           7     0.6446    0.5292    0.5812      3012\n",
      "           8     0.6069    0.6636    0.6340      2982\n",
      "           9     0.7500    0.6087    0.6720      2982\n",
      "\n",
      "    accuracy                         0.5888     30000\n",
      "   macro avg     0.6324    0.5890    0.5963     30000\n",
      "weighted avg     0.6328    0.5888    0.5963     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.983216641019145\n",
      "CCA coefficients mean non-concern: 0.9787582271339205\n",
      "Linear CKA concern: 0.9341859323626184\n",
      "Linear CKA non-concern: 0.9007985643659974\n",
      "Kernel CKA concern: 0.8990570355013408\n",
      "Kernel CKA non-concern: 0.8389024385290561\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.5931427478790283\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[1.8432e-03, 1.4195e-03, 1.2978e-03, 1.3643e-03],\n",
      "        [1.4305e-03, 1.4984e-03, 1.5936e-03, 1.4440e-03],\n",
      "        [1.0138e-03, 1.6030e-03, 1.1575e-03, 9.0450e-04],\n",
      "        [1.6730e+00, 2.1616e+00, 2.3961e+00, 2.3196e+00]])\n",
      "head_score\n",
      " tensor([[0.6158, 0.4743, 0.4336, 0.4558],\n",
      "        [0.4791, 0.5018, 0.5337, 0.4836],\n",
      "        [0.4226, 0.6682, 0.4825, 0.3770],\n",
      "        [0.3880, 0.5013, 0.5557, 0.5379]])\n",
      "[(2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[1.6861e-03, 1.4158e-03, 1.2621e-03, 1.3538e-03],\n",
      "        [1.4646e-03, 1.5000e-03, 1.5816e-03, 1.4565e-03],\n",
      "        [1.0232e-03, 1.6237e-03, 1.1521e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3082e+00, 2.5401e+00, 2.4948e+00]])\n",
      "head_score\n",
      " tensor([[0.5862, 0.4922, 0.4388, 0.4707],\n",
      "        [0.4877, 0.4995, 0.5267, 0.4850],\n",
      "        [0.4571, 0.7254, 0.5147, 0.0000],\n",
      "        [0.0000, 0.5440, 0.5986, 0.5880]])\n",
      "[(0, 2), (2, 0), (2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[1.7350e-03, 1.5593e-03, 0.0000e+00, 1.5582e-03],\n",
      "        [1.6802e-03, 1.4972e-03, 1.7096e-03, 1.6514e-03],\n",
      "        [0.0000e+00, 1.7140e-03, 1.2603e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.5088e+00, 2.5922e+00, 2.6122e+00]])\n",
      "head_score\n",
      " tensor([[0.6185, 0.5558, 0.0000, 0.5555],\n",
      "        [0.5133, 0.4574, 0.5223, 0.5045],\n",
      "        [0.0000, 0.8057, 0.5924, 0.0000],\n",
      "        [0.0000, 0.5633, 0.5820, 0.5865]])\n",
      "[(0, 2), (1, 1), (1, 3), (2, 0), (2, 3), (3, 0)]\n",
      "head_importance_list\n",
      " tensor([[1.8784e-03, 1.6932e-03, 0.0000e+00, 1.7455e-03],\n",
      "        [2.1531e-03, 0.0000e+00, 2.2510e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5636e-03, 1.5237e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9671e+00, 2.6184e+00, 2.7267e+00]])\n",
      "head_score\n",
      " tensor([[0.6113, 0.5510, 0.0000, 0.5680],\n",
      "        [0.6912, 0.0000, 0.7227, 0.0000],\n",
      "        [0.0000, 0.7162, 0.6979, 0.0000],\n",
      "        [0.0000, 0.6174, 0.5449, 0.5674]])\n",
      "[(0, 1), (0, 2), (1, 1), (1, 3), (2, 0), (2, 3), (3, 0), (3, 2)]\n",
      "Evaluate the pruned model 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844558a1afec4bf68f1d48562c395fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3242\n",
      "Precision: 0.6320, Recall: 0.5898, F1-Score: 0.5942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3995    0.6120    0.4834      2992\n",
      "           1     0.6433    0.4786    0.5489      2992\n",
      "           2     0.6873    0.5495    0.6107      3012\n",
      "           3     0.3572    0.5927    0.4458      2998\n",
      "           4     0.7909    0.6361    0.7051      2973\n",
      "           5     0.7619    0.7911    0.7762      3054\n",
      "           6     0.7523    0.3287    0.4575      3003\n",
      "           7     0.5712    0.6152    0.5924      3012\n",
      "           8     0.6766    0.5936    0.6324      2982\n",
      "           9     0.6796    0.7005    0.6899      2982\n",
      "\n",
      "    accuracy                         0.5900     30000\n",
      "   macro avg     0.6320    0.5898    0.5942     30000\n",
      "weighted avg     0.6321    0.5900    0.5944     30000\n",
      "\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "adding eps to diagonal and taking inverse\n",
      "taking square root\n",
      "dot products...\n",
      "trying to take final svd\n",
      "computed everything!\n",
      "CCA coefficients mean concern: 0.9831847877244927\n",
      "CCA coefficients mean non-concern: 0.9790312536776709\n",
      "Linear CKA concern: 0.903650281854233\n",
      "Linear CKA non-concern: 0.9289607781519111\n",
      "Kernel CKA concern: 0.844304538028652\n",
      "Kernel CKA non-concern: 0.8693527744253681\n",
      "original model's perplexity\n",
      "3.2110652923583984\n",
      "pruned model's perplexity\n",
      "3.5726349353790283\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "\n",
    "for concern in range(config.num_labels):\n",
    "  config.init_seed()\n",
    "  positive_samples = SamplingDataset(\n",
    "      train_dataloader,\n",
    "      config,\n",
    "      concern,\n",
    "      num_samples,\n",
    "      True,\n",
    "      4,\n",
    "      resample=False,\n",
    "  )\n",
    "  negative_samples = SamplingDataset(\n",
    "      train_dataloader,\n",
    "      config,\n",
    "      concern,\n",
    "      num_samples,\n",
    "      False,\n",
    "      4,\n",
    "      resample=False,\n",
    "  )\n",
    "  module = copy.deepcopy(model)\n",
    "\n",
    "  head_importance_prunning(module, config, positive_samples, ratio)\n",
    "\n",
    "  print(f\"Evaluate the pruned model {concern}\")\n",
    "  result = evaluate_model(module, config, test_dataloader)\n",
    "  result_list.append(result)\n",
    "  get_similarity(model, module, valid_dataloader, concern, num_samples, config)\n",
    "  print(\"original model's perplexity\")\n",
    "  get_perplexity(model, valid_dataloader, config)\n",
    "  print(\"pruned model's perplexity\")\n",
    "  get_perplexity(module, valid_dataloader, config)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-04_00-55-15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4406</td>\n",
       "      <td>0.5973</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6342</td>\n",
       "      <td>0.4873</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.6421</td>\n",
       "      <td>0.6451</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>3012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3205</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>0.7999</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.7912</td>\n",
       "      <td>0.7705</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>3054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.6396</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.6394</td>\n",
       "      <td>0.6245</td>\n",
       "      <td>3012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.6069</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>2982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>2982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  precision  recall  f1-score  support\n",
       "0     0     0.4406  0.5973    0.5071     2992\n",
       "1     1     0.6342  0.4873    0.5511     2992\n",
       "2     2     0.6421  0.6451    0.6436     3012\n",
       "3     3     0.3205  0.6468    0.4286     2998\n",
       "4     4     0.6790  0.7999    0.7345     2973\n",
       "5     5     0.7912  0.7705    0.7807     3054\n",
       "6     6     0.6396  0.4279    0.5128     3003\n",
       "7     7     0.6103  0.6394    0.6245     3012\n",
       "8     8     0.6069  0.6636    0.6340     2982\n",
       "9     9     0.6796  0.7005    0.6899     2982"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.helper import report_to_df, append_nth_row\n",
    "df_list = [report_to_df(df) for df in result_list]\n",
    "new_df = append_nth_row(df_list)\n",
    "csv_name = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "new_df.to_csv(f\"results/{csv_name}.csv\", index=False)\n",
    "print(csv_name)\n",
    "new_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decomposetransformer-UESb9BbT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
