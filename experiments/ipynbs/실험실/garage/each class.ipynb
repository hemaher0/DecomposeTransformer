{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "452f1e71-25d2-4e1c-8adf-a67975b01a05",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"../../../\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa3ba38f-4f00-4850-b877-3440ffc6a0bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from src.utils.helper import Config, color_print\n",
        "from src.utils.load import load_model, load_data, save_checkpoint\n",
        "from src.utils.load import save_checkpoint\n",
        "from src.models.evaluate import evaluate_model, get_sparsity, get_similarity\n",
        "from src.utils.sampling import SamplingDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee111d1c-e7cc-4098-b8bd-3c6f30e25112",
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"bert-4-128-yahoo\"\n",
        "# name = \"YahooAnswersTopics\"\n",
        "device = torch.device(\"cuda:0\")\n",
        "checkpoint = None\n",
        "batch_size = 16\n",
        "num_workers = 4\n",
        "num_samples = 128\n",
        "head_pruning_ratio = 0.5\n",
        "seed = 44\n",
        "include_layers = [\"attention\", \"intermediate\", \"output\"]\n",
        "        exclude_layers=exclude_layers,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb73bcd-747e-4eb9-8d63-9f81fb4f175e",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(name, device)\n",
        "num_labels = config.config[\"num_labels\"]\n",
        "model = load_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa58a214-8c91-4b80-b9f2-127ea8ddded5",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
        "    config,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    do_cache=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bb913ce9-7104-4baa-83b7-cb1ab2675396",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "from transformers.pytorch_utils import find_pruneable_heads_and_indices\n",
        "from typing import *\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def calculate_prune_head(arr, i, pruned_heads):\n",
        "    flattened_with_indices = [\n",
        "        (value, index)\n",
        "        for index, value in np.ndenumerate(arr)\n",
        "        if index not in pruned_heads\n",
        "    ]\n",
        "\n",
        "    sorted_by_value = sorted(flattened_with_indices, key=lambda x: x[0])\n",
        "    bottom_indices = sorted_by_value[:i]\n",
        "\n",
        "    bottom_indices_only = [index for _, index in bottom_indices]\n",
        "\n",
        "    return bottom_indices_only\n",
        "\n",
        "\n",
        "def prune_head(model, prune_list):\n",
        "    for layer_index, head_index in prune_list:\n",
        "        prune_heads(model.bert.encoder.layer[layer_index].attention, ([head_index]))\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_prunehead(arr, num_layer):\n",
        "    layer_max = lambda arr: np.argmax(arr, axis=1)\n",
        "\n",
        "    max_layer = layer_max(arr)\n",
        "    for layer in range(num_layer):\n",
        "        head = max_layer[layer]\n",
        "        arr[layer][head] = 100\n",
        "    return arr\n",
        "\n",
        "\n",
        "def prune_heads(layer, heads):\n",
        "    if len(heads) == 0:\n",
        "        return\n",
        "    heads, index = find_pruneable_heads_and_indices(\n",
        "        heads,\n",
        "        layer.self.num_attention_heads,\n",
        "        layer.self.attention_head_size,\n",
        "        layer.pruned_heads,\n",
        "    )\n",
        "\n",
        "    # Zero out weights in linear layers instead of pruning\n",
        "    layer.self.query = zero_out_head_weights(\n",
        "        layer.self.query, heads, layer.self.attention_head_size\n",
        "    )\n",
        "    layer.self.key = zero_out_head_weights(\n",
        "        layer.self.key, heads, layer.self.attention_head_size\n",
        "    )\n",
        "    layer.self.value = zero_out_head_weights(\n",
        "        layer.self.value, heads, layer.self.attention_head_size\n",
        "    )\n",
        "    layer.output.dense = zero_out_head_weights(\n",
        "        layer.output.dense, heads, layer.self.attention_head_size, dim=1\n",
        "    )\n",
        "\n",
        "\n",
        "def zero_out_head_weights(\n",
        "    layer: nn.Linear, heads: Set[int], head_size: int, dim: int = 0\n",
        ") -> nn.Linear:\n",
        "    \"\"\"\n",
        "    Zero out the weights of the specified heads in the linear layer.\n",
        "\n",
        "    Args:\n",
        "        layer (`torch.nn.Linear`): The layer to modify.\n",
        "        heads (`Set[int]`): The indices of heads to zero out.\n",
        "        head_size (`int`): The size of each head.\n",
        "        dim (`int`, *optional*, defaults to 0): The dimension on which to zero out the weights.\n",
        "\n",
        "    Returns:\n",
        "        `torch.nn.Linear`: The modified layer with weights of specified heads zeroed out.\n",
        "    \"\"\"\n",
        "    for head in heads:\n",
        "        start_index = head * head_size\n",
        "        end_index = (head + 1) * head_size\n",
        "        if dim == 0:\n",
        "            layer.weight.data[start_index:end_index] = 0\n",
        "        elif dim == 1:\n",
        "            layer.weight.data[:, start_index:end_index] = 0\n",
        "\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7d624617-8272-4bea-b5fe-01481b311567",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_head_importance(\n",
        "    model,\n",
        "    config,\n",
        "    data,\n",
        "    normalize_scores_by_layer=True,\n",
        "):\n",
        "    device = config.device\n",
        "    from functools import partial\n",
        "\n",
        "    gradients = {}\n",
        "    context_layers = {}\n",
        "\n",
        "    def save_grad(gradients, layer_idx, grad):\n",
        "        gradients[f\"context_layer_{layer_idx}\"] = grad\n",
        "\n",
        "    def forward_hook(module, input, output, gradients, context_layers, layer_idx):\n",
        "        context_layers[f\"context_layer_{layer_idx}\"] = output[0]\n",
        "        output[0].register_hook(partial(save_grad, gradients, layer_idx))\n",
        "\n",
        "    def reshape(tensors, shape, num_heads):\n",
        "        batch_size = shape[0]\n",
        "        seq_len = shape[1]\n",
        "        head_dim = shape[2] // num_heads\n",
        "        tensors = tensors.reshape(batch_size, seq_len, num_heads, head_dim)\n",
        "        tensors = tensors.permute(0, 2, 1, 3)\n",
        "        return tensors\n",
        "\n",
        "    forward_handles = []\n",
        "\n",
        "    for layer_idx in range(model.bert.config.num_hidden_layers):\n",
        "        self_att = model.bert.encoder.layer[layer_idx].attention.self\n",
        "        handle = self_att.register_forward_hook(\n",
        "            partial(\n",
        "                forward_hook,\n",
        "                gradients=gradients,\n",
        "                context_layers=context_layers,\n",
        "                layer_idx=layer_idx,\n",
        "            )\n",
        "        )\n",
        "        forward_handles.append(handle)\n",
        "\n",
        "    # Disable dropout\n",
        "    model.eval()\n",
        "    device = device or next(model.parameters()).device\n",
        "\n",
        "    n_layers = model.bert.config.num_hidden_layers\n",
        "    n_heads = model.bert.config.num_attention_heads\n",
        "    head_dim = model.bert.config.hidden_size // n_heads\n",
        "    num_classes = model.num_labels  # Adjust based on your model\n",
        "\n",
        "    head_importance = {\n",
        "        label: torch.zeros(n_layers, n_heads).to(device) for label in range(num_classes)\n",
        "    }\n",
        "    tot_tokens = {label: 0 for label in range(num_classes)}\n",
        "\n",
        "    for step, batch in enumerate(data):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        input_mask = batch[\"attention_mask\"].to(device)\n",
        "        label_ids = batch[\"labels\"].to(device)\n",
        "        unique_labels = label_ids.unique()\n",
        "\n",
        "        for label in unique_labels:\n",
        "            mask = label_ids == label\n",
        "            input_ids_label = input_ids[mask]\n",
        "            input_mask_label = input_mask[mask]\n",
        "            label_ids_label = label_ids[mask]\n",
        "\n",
        "            if input_ids_label.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            # Zero gradients\n",
        "            model.zero_grad()\n",
        "            # Compute loss and backward pass\n",
        "            loss = model(\n",
        "                input_ids_label, attention_mask=input_mask_label, labels=label_ids_label\n",
        "            ).loss\n",
        "            loss.backward()\n",
        "\n",
        "            for layer_idx in range(n_layers):\n",
        "                ctx = context_layers[f\"context_layer_{layer_idx}\"]\n",
        "                grad_ctx = gradients[f\"context_layer_{layer_idx}\"]\n",
        "                shape = ctx.shape\n",
        "                ctx = reshape(ctx, shape, n_heads)\n",
        "                grad_ctx = reshape(grad_ctx, shape, n_heads)\n",
        "\n",
        "                # Compute dot product and accumulate head importance per class\n",
        "                dot = torch.einsum(\"bhli,bhli->bhl\", [grad_ctx, ctx])\n",
        "                head_importance[label.item()][layer_idx] += (\n",
        "                    dot.abs().sum(-1).sum(0).detach()\n",
        "                )\n",
        "                del ctx, grad_ctx, dot\n",
        "\n",
        "            tot_tokens[label.item()] += input_mask_label.float().detach().sum().item()\n",
        "\n",
        "    for label in range(num_classes):\n",
        "        # Adjust the value weight norm addition\n",
        "        for layer_idx in range(n_layers):\n",
        "            for head in range(n_heads):\n",
        "                start_idx = head * head_dim\n",
        "                end_idx = (head + 1) * head_dim\n",
        "\n",
        "                value_weight_norm = torch.norm(\n",
        "                    model.bert.encoder.layer[layer_idx].attention.self.value.weight[\n",
        "                        :, start_idx:end_idx\n",
        "                    ]\n",
        "                )\n",
        "                head_importance[label][layer_idx][head] += value_weight_norm.detach()\n",
        "\n",
        "        # Normalize head importance per class\n",
        "        head_importance[label][:-1] /= (\n",
        "            tot_tokens[label] + 1e-20\n",
        "        )  # Avoid division by zero\n",
        "\n",
        "        if normalize_scores_by_layer:\n",
        "            exponent = 2\n",
        "            norm_by_layer = torch.pow(\n",
        "                torch.pow(head_importance[label], exponent).sum(-1), 1 / exponent\n",
        "            )\n",
        "            head_importance[label] /= norm_by_layer.unsqueeze(-1) + 1e-20\n",
        "\n",
        "    for handle in forward_handles:\n",
        "        handle.remove()\n",
        "\n",
        "    return head_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9c67185e-f323-4e0f-b2e3-b58f3fd060d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_samples = SamplingDataset(\n",
        "    train_dataloader,\n",
        "    0,\n",
        "    num_samples,\n",
        "    num_labels,\n",
        "    True,\n",
        "    4,\n",
        "    device=device,\n",
        "    resample=False,\n",
        ")\n",
        "all_samples = SamplingDataset(\n",
        "    train_dataloader,\n",
        "    200,\n",
        "    num_samples,\n",
        "    num_labels,\n",
        "    False,\n",
        "    4,\n",
        "    device=device,\n",
        "    resample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5ebd1440-411c-4282-a14a-3cf6d4e991a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def head_importance_prunning(\n",
        "    model, config, dominant_concern, concern, sparsity_ratio, gradually=True\n",
        "):\n",
        "    num_attention_heads = model.config.num_attention_heads\n",
        "    num_hidden_layers = model.config.num_hidden_layers\n",
        "    total_heads_to_prune = int(num_attention_heads * num_hidden_layers * sparsity_ratio)\n",
        "\n",
        "    if total_heads_to_prune >= 4 and total_heads_to_prune % 4 != 0:\n",
        "        total_heads_to_prune -= 4 - (total_heads_to_prune % 4)\n",
        "\n",
        "    if gradually:\n",
        "        num_steps = max(1, total_heads_to_prune // 4)\n",
        "    else:\n",
        "        num_steps = 1\n",
        "\n",
        "    heads_per_step = int(total_heads_to_prune // num_steps)\n",
        "    print(f\"Total heads to prune: {total_heads_to_prune}\")\n",
        "\n",
        "    pruned_heads = set()\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        if step == num_steps - 1:\n",
        "            current_heads_to_prune = total_heads_to_prune - (step * heads_per_step)\n",
        "        else:\n",
        "            current_heads_to_prune = heads_per_step\n",
        "\n",
        "        head_importance_list = calculate_head_importance(\n",
        "            model,\n",
        "            config,\n",
        "            dominant_concern,\n",
        "            normalize_scores_by_layer=True,\n",
        "        )\n",
        "        head_importance_list = head_importance_list[concern]\n",
        "        print(f\"head importance list\\n {head_importance_list}\")\n",
        "        head_importance_list = head_importance_list.cpu()\n",
        "\n",
        "        # preprocess_prunehead(head_importance_list, num_hidden_layers)\n",
        "\n",
        "        prune_list = calculate_prune_head(\n",
        "            head_importance_list, current_heads_to_prune, pruned_heads\n",
        "        )\n",
        "        pruned_heads.update(prune_list)\n",
        "\n",
        "        prune_head(model, prune_list)\n",
        "    print(pruned_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d839b6db-2015-44f4-8cac-624e0b8686ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "module = copy.deepcopy(model)\n",
        "head_importance_prunning(module, config, all_samples, 0, head_pruning_ratio)\n",
        "result = evaluate_model(module, config, test_dataloader, verbose=True)\n",
        "get_similarity(\n",
        "    model,\n",
        "    module,\n",
        "    valid_dataloader,\n",
        "    0,\n",
        "    num_samples,\n",
        "    num_labels,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2babaaff",
      "metadata": {},
      "outputs": [],
      "source": [
        "for concern in range(num_labels):\n",
        "    train = copy.deepcopy(train_dataloader)\n",
        "    positive_samples = SamplingDataset(\n",
        "        train,\n",
        "        concern,\n",
        "        num_samples,\n",
        "        num_labels,\n",
        "        True,\n",
        "        4,\n",
        "        device=device,\n",
        "        resample=False,\n",
        "    )\n",
        "\n",
        "    module = copy.deepcopy(model)\n",
        "    head_importance_prunning(\n",
        "        module, config, positive_samples, concern, head_pruning_ratio\n",
        "    )\n",
        "    print(f\"Evaluate the pruned model {concern}\")\n",
        "\n",
        "    result = evaluate_model(module, config, test_dataloader, verbose=True)\n",
        "    get_similarity(\n",
        "        model,\n",
        "        module,\n",
        "        valid_dataloader,\n",
        "        0,\n",
        "        num_samples,\n",
        "        num_labels,\n",
        "        device=device,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}