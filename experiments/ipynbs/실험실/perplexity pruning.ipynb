{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from src.utils.helper import Config, color_print\n",
    "from src.utils.load import load_model, load_data, save_checkpoint, load_checkpoint\n",
    "from src.models.evaluate import evaluate_model, get_sparsity, get_similarity\n",
    "from src.utils.sampling import SamplingDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"bert-4-128-yahoo\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "checkpoint = None\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "num_samples = 128\n",
    "ratio = 0.5\n",
    "seed = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model.\n",
      "{'architectures': 'bert',\n",
      " 'dataset_name': 'YahooAnswersTopics',\n",
      " 'model_name': 'models/bert-4-128-yahoo',\n",
      " 'num_labels': 10,\n",
      " 'tokenizer_name': 'fabriceyhc/bert-base-uncased-yahoo_answers_topics'}\n",
      "The model models/bert-4-128-yahoo is loaded.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset YahooAnswersTopics.\n",
      "train.pkl is loaded from cache.\n",
      "valid.pkl is loaded from cache.\n",
      "test.pkl is loaded from cache.\n",
      "The dataset YahooAnswersTopics is loaded\n",
      "{'config_name': 'yahoo_answers_topics',\n",
      " 'features': {'first_column': 'question_title', 'second_column': 'topic'},\n",
      " 'path': 'yahoo_answers_topics'}\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
    "    config,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    do_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import *\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from transformers.pytorch_utils import (\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_head_importance(\n",
    "    model,\n",
    "    config,\n",
    "    data,\n",
    "    normalize_scores_by_layer=True,\n",
    "):\n",
    "    device = config.device\n",
    "    from functools import partial\n",
    "\n",
    "    gradients = {}\n",
    "    context_layers = {}\n",
    "\n",
    "    def save_grad(gradients, layer_idx, grad):\n",
    "        gradients[f\"context_layer_{layer_idx}\"] = grad\n",
    "\n",
    "    def forward_hook(module, input, output, gradients, context_layers, layer_idx):\n",
    "        context_layers[f\"context_layer_{layer_idx}\"] = output[0]\n",
    "        output[0].register_hook(partial(save_grad, gradients, layer_idx))\n",
    "\n",
    "    def reshape(tensors, shape, num_heads):\n",
    "        batch_size = shape[0]\n",
    "        seq_len = shape[1]\n",
    "        head_dim = shape[2] // num_heads\n",
    "        tensors = tensors.reshape(batch_size, seq_len, num_heads, head_dim)\n",
    "        tensors = tensors.permute(0, 2, 1, 3)\n",
    "        return tensors\n",
    "\n",
    "    forward_handles = []\n",
    "\n",
    "    for layer_idx in range(model.bert.config.num_hidden_layers):\n",
    "        self_att = model.bert.encoder.layer[layer_idx].attention.self\n",
    "        handle = self_att.register_forward_hook(\n",
    "            partial(\n",
    "                forward_hook,\n",
    "                gradients=gradients,\n",
    "                context_layers=context_layers,\n",
    "                layer_idx=layer_idx,\n",
    "            )\n",
    "        )\n",
    "        forward_handles.append(handle)\n",
    "\n",
    "    \"\"\"Calculate head importance scores\"\"\"\n",
    "    # Disable dropout\n",
    "    model.eval()\n",
    "    # Device\n",
    "    device = device or next(model.parameters()).device\n",
    "\n",
    "    # Prepare data loader\n",
    "    # Head importance tensor\n",
    "    n_layers = model.bert.config.num_hidden_layers\n",
    "    n_heads = model.bert.config.num_attention_heads\n",
    "    num_batches = 0\n",
    "    head_dim = model.bert.config.hidden_size // n_heads\n",
    "    head_importance = torch.zeros(n_layers, n_heads).to(device)\n",
    "    tot_tokens = 0\n",
    "    first_batch = next(iter(data))\n",
    "    is_embeds = \"embeddings\" in first_batch\n",
    "    for step, batch in enumerate(data):\n",
    "        num_batches += 1\n",
    "        if is_embeds:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "        else:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        label_ids = batch[\"labels\"].to(device)\n",
    "        # Compute gradients\n",
    "        if is_embeds:\n",
    "            loss = model(\n",
    "                inputs_embeds=embeddings, attention_mask=input_mask, labels=label_ids\n",
    "            ).loss\n",
    "        else:\n",
    "            loss = model(input_ids, attention_mask=input_mask, labels=label_ids).loss\n",
    "        loss.backward()\n",
    "\n",
    "        for layer_idx in range(model.bert.config.num_hidden_layers):\n",
    "            ctx = context_layers[f\"context_layer_{layer_idx}\"]\n",
    "            grad_ctx = gradients[f\"context_layer_{layer_idx}\"]\n",
    "            shape = ctx.shape\n",
    "            ctx = reshape(ctx, shape, n_heads)\n",
    "            grad_ctx = reshape(grad_ctx, shape, n_heads)\n",
    "\n",
    "            # Take the dot\n",
    "            dot = torch.einsum(\"bhli,bhli->bhl\", [grad_ctx, ctx])\n",
    "            head_importance[layer_idx] += dot.abs().sum(-1).sum(0).detach()\n",
    "            del ctx, grad_ctx, dot\n",
    "\n",
    "        tot_tokens += input_mask.float().detach().sum().data\n",
    "\n",
    "    head_importance[:-1] /= tot_tokens\n",
    "    head_importance /= num_batches\n",
    "    for handle in forward_handles:\n",
    "        handle.remove()\n",
    "    return head_importance\n",
    "\n",
    "def calculate_head_loss(model, config, data):\n",
    "    device = config.device\n",
    "    model.eval()\n",
    "    n_layers = model.bert.config.num_hidden_layers\n",
    "    n_heads = model.bert.config.num_attention_heads\n",
    "    is_embeds = \"embeddings\" in next(iter(data))\n",
    "\n",
    "    head_losses = torch.zeros(n_layers, n_heads).to(device)\n",
    "    num_samples = 0\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "        if is_embeds:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "        else:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = input_mask.size(0)\n",
    "        \n",
    "        for batch_item in range(batch_size):\n",
    "            num_samples += 1\n",
    "            for layer in range(n_layers):\n",
    "                for head in range(n_heads):\n",
    "                    head_mask = torch.ones(n_layers, n_heads).to(device)\n",
    "                    head_mask[layer, head] = 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        if is_embeds:\n",
    "                            outputs = model(inputs_embeds=embeddings, attention_mask=input_mask, head_mask=head_mask, output_attentions=True)\n",
    "                        else:                \n",
    "                            outputs = model(input_ids=input_ids, attention_mask=input_mask, head_mask=head_mask, output_attentions=True)\n",
    "                            \n",
    "                    logits = outputs.logits\n",
    "                    criterion = torch.nn.CrossEntropyLoss()\n",
    "                    loss = criterion(logits[batch_item].unsqueeze(0), labels[batch_item].unsqueeze(0))\n",
    "                    head_losses[layer, head] += loss.item()\n",
    "\n",
    "    avg_loss = head_losses / num_samples\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_layer_loss(model, config, data):\n",
    "    device = config.device\n",
    "    model.eval()\n",
    "    n_layers = model.bert.config.num_hidden_layers\n",
    "    n_heads = model.bert.config.num_attention_heads\n",
    "    is_embeds = \"embeddings\" in next(iter(data))\n",
    "\n",
    "    layer_losses = torch.zeros(n_layers).to(device)\n",
    "    num_samples = 0\n",
    "\n",
    "    for step, batch in enumerate(data):\n",
    "        if is_embeds:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "        else:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = input_mask.size(0)\n",
    "        \n",
    "        for batch_item in range(batch_size):\n",
    "            num_samples += 1\n",
    "            for layer in range(n_layers):\n",
    "            \n",
    "                head_mask = torch.ones(n_layers).to(device)\n",
    "                head_mask[layer] = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if is_embeds:\n",
    "                        outputs = model(inputs_embeds=embeddings, attention_mask=input_mask, head_mask=head_mask, output_attentions=True)\n",
    "                    else:                \n",
    "                        outputs = model(input_ids=input_ids, attention_mask=input_mask, head_mask=head_mask, output_attentions=True)\n",
    "                        \n",
    "                logits = outputs.logits\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                loss = criterion(logits[batch_item].unsqueeze(0), labels[batch_item].unsqueeze(0))\n",
    "                layer_losses[layer] += loss.item()\n",
    "\n",
    "    avg_loss = layer_losses / num_samples\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def normalize(tensors):\n",
    "    exponent = 2\n",
    "    norm_by_layer = torch.pow(\n",
    "        torch.pow(tensors, exponent).sum(-1), 1 / exponent\n",
    "    )\n",
    "    tensors /= norm_by_layer.unsqueeze(-1) + 1e-20\n",
    "    return tensors\n",
    "\n",
    "def head_importance_prunning(\n",
    "    model, config, dominant_concern, sparsity_ratio, method=\"unstructed\", scheduler=None\n",
    "):\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    num_hidden_layers = model.config.num_hidden_layers\n",
    "    model = model.to(config.device)\n",
    "    total_heads_to_prune = int(num_attention_heads * num_hidden_layers * sparsity_ratio)\n",
    "\n",
    "    total_heads_to_prune = max(total_heads_to_prune, num_hidden_layers)\n",
    "    print(f\"Total heads to prune: {total_heads_to_prune}\")\n",
    "    pruned_heads = set()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        steps = scheduler.get_steps()\n",
    "    else:\n",
    "        steps = [0.25, 0.25, 0.25, 0.25]\n",
    "        # steps = [1.0]\n",
    "\n",
    "    for step_ratio in steps:\n",
    "        heads_to_prune = int(total_heads_to_prune * step_ratio)\n",
    "        \n",
    "        head_importance_list = calculate_head_importance(\n",
    "            model, config, dominant_concern\n",
    "        )\n",
    "        head_loss = calculate_head_loss(\n",
    "            model, config, dominant_concern\n",
    "        )\n",
    "        layer_loss = calculate_layer_loss(\n",
    "            model, config, dominant_concern\n",
    "        )\n",
    "        head_importance_list = head_importance_list.cpu()\n",
    "        head_loss = head_loss.cpu()\n",
    "        layer_loss = layer_loss.cpu()\n",
    "        # head_importance_list = normalize(head_importance_list)\n",
    "        # head_loss = normalize(head_loss)\n",
    "        # layer_loss = normalize(layer_loss)\n",
    "        head_importance_list = torch.log1p(head_importance_list)\n",
    "        print(f\"head_importance_list\\n {head_importance_list}\")\n",
    "        print(f\"head_loss\\n {head_loss}\")\n",
    "        print(f\"layer_loss\\n {layer_loss}\")\n",
    "        alpha = head_loss * layer_loss.unsqueeze(1)\n",
    "        print(alpha)\n",
    "\n",
    "        head_score = head_importance_list * alpha\n",
    "        print(f\"head_score\\n {head_score}\")\n",
    "        if method == \"unstructed\":\n",
    "            sorted_indices = torch.argsort(head_score.view(-1))\n",
    "            prune_list = []\n",
    "            for idx in sorted_indices:\n",
    "                layer_index = int(idx // num_attention_heads)\n",
    "                head_index = int(idx % num_attention_heads)\n",
    "\n",
    "                if (layer_index, head_index) not in pruned_heads:\n",
    "                    prune_list.append((layer_index, head_index))\n",
    "                \n",
    "                if len(prune_list) >= heads_to_prune:\n",
    "                    break\n",
    "            \n",
    "        elif method == \"structed\":\n",
    "            heads_per_layer = heads_to_prune // num_hidden_layers\n",
    "            prune_list = []\n",
    "            for layer_idx in range(num_hidden_layers):\n",
    "                sorted_heads = torch.argsort(head_importance_list[layer_idx])\n",
    "                prune_list.extend(\n",
    "                    [\n",
    "                        (layer_idx, head.item())\n",
    "                        for head in sorted_heads[:heads_per_layer]\n",
    "                    ]\n",
    "                )\n",
    "        for layer_index, head_index in prune_list:\n",
    "            if (layer_index, head_index) not in pruned_heads:\n",
    "                prune_heads(\n",
    "                    model.bert.encoder.layer[layer_index].attention,\n",
    "                    [head_index],\n",
    "                    method=method,\n",
    "                )\n",
    "                pruned_heads.add((layer_index, head_index))\n",
    "        print(sorted(pruned_heads))\n",
    "\n",
    "\n",
    "def prune_heads(layer, heads, method):\n",
    "    if len(heads) == 0:\n",
    "        return\n",
    "    heads, index = find_pruneable_heads_and_indices(\n",
    "        heads,\n",
    "        layer.self.num_attention_heads,\n",
    "        layer.self.attention_head_size,\n",
    "        layer.pruned_heads,\n",
    "    )\n",
    "\n",
    "    # if method == \"unstructed\":\n",
    "    layer.self.query = zero_out_head_weights(\n",
    "        layer.self.query, heads, layer.self.attention_head_size\n",
    "    )\n",
    "    layer.self.key = zero_out_head_weights(\n",
    "        layer.self.key, heads, layer.self.attention_head_size\n",
    "    )\n",
    "    layer.self.value = zero_out_head_weights(\n",
    "        layer.self.value, heads, layer.self.attention_head_size\n",
    "    )\n",
    "    layer.output.dense = zero_out_head_weights(\n",
    "        layer.output.dense, heads, layer.self.attention_head_size, dim=1\n",
    "    )\n",
    "    # elif method == \"structed\":\n",
    "    #     layer.self.query = prune_linear_layer(layer.self.query, index)\n",
    "    #     layer.self.key = prune_linear_layer(layer.self.key, index)\n",
    "    #     layer.self.value = prune_linear_layer(layer.self.value, index)\n",
    "    #     layer.output.dense = prune_linear_layer(layer.output.dense, index)\n",
    "\n",
    "    #     layer.self.num_attention_heads = layer.self.num_attention_heads - len(heads)\n",
    "    #     layer.self.all_head_size = layer.self.attention_head_size *  layer.self.num_attention_heads\n",
    "    #     layer.pruned_heads = layer.pruned_heads.union(heads)\n",
    "\n",
    "\n",
    "def zero_out_head_weights(\n",
    "    layer: nn.Linear, heads: Set[int], head_size: int, dim: int = 0\n",
    ") -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Zero out the weights of the specified heads in the linear layer.\n",
    "\n",
    "    Args:\n",
    "        layer (`torch.nn.Linear`): The layer to modify.\n",
    "        heads (`Set[int]`): The indices of heads to zero out.\n",
    "        head_size (`int`): The size of each head.\n",
    "        dim (`int`, *optional*, defaults to 0): The dimension on which to zero out the weights.\n",
    "\n",
    "    Returns:\n",
    "        `torch.nn.Linear`: The modified layer with weights of specified heads zeroed out.\n",
    "    \"\"\"\n",
    "    for head in heads:\n",
    "        start_index = head * head_size\n",
    "        end_index = (head + 1) * head_size\n",
    "        if dim == 0:\n",
    "            layer.weight.data[start_index:end_index] = 0\n",
    "        elif dim == 1:\n",
    "            layer.weight.data[:, start_index:end_index] = 0\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.evaluate import (\n",
    "    evaluate_model,\n",
    "    get_sparsity,\n",
    "    get_similarity,\n",
    "    get_perplexity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total heads to prune: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_importance_list\n",
      " tensor([[5.6371e-05, 6.4266e-05, 5.7026e-05, 5.8088e-05],\n",
      "        [4.8789e-05, 8.0193e-05, 6.4501e-05, 6.9048e-05],\n",
      "        [5.0231e-05, 7.1948e-05, 5.1353e-05, 5.3254e-05],\n",
      "        [5.9122e-02, 4.9883e-02, 6.9143e-02, 5.9519e-02]])\n",
      "head_loss\n",
      " tensor([[1.4575, 1.4098, 1.4667, 1.5068],\n",
      "        [1.4333, 1.4087, 1.4519, 1.4669],\n",
      "        [1.4812, 1.4692, 1.4888, 1.4394],\n",
      "        [1.5510, 1.4956, 1.4531, 1.4402]])\n",
      "layer_loss\n",
      " tensor([1.4241, 1.3157, 1.3873, 1.3975])\n",
      "tensor([[2.0757, 2.0078, 2.0887, 2.1459],\n",
      "        [1.8858, 1.8534, 1.9101, 1.9299],\n",
      "        [2.0548, 2.0382, 2.0654, 1.9969],\n",
      "        [2.1675, 2.0901, 2.0307, 2.0127]])\n",
      "head_score\n",
      " tensor([[1.1701e-04, 1.2903e-04, 1.1911e-04, 1.2465e-04],\n",
      "        [9.2005e-05, 1.4863e-04, 1.2321e-04, 1.3325e-04],\n",
      "        [1.0321e-04, 1.4664e-04, 1.0607e-04, 1.0634e-04],\n",
      "        [1.2815e-01, 1.0426e-01, 1.4041e-01, 1.1980e-01]])\n",
      "[(1, 0), (2, 0)]\n",
      "head_importance_list\n",
      " tensor([[5.5054e-05, 6.4347e-05, 5.6189e-05, 5.9340e-05],\n",
      "        [0.0000e+00, 8.1570e-05, 6.4424e-05, 6.9874e-05],\n",
      "        [0.0000e+00, 7.3588e-05, 5.4377e-05, 5.6000e-05],\n",
      "        [5.5955e-02, 5.0401e-02, 7.1206e-02, 6.1109e-02]])\n",
      "head_loss\n",
      " tensor([[1.3765, 1.3387, 1.3892, 1.4334],\n",
      "        [1.4184, 1.3291, 1.3723, 1.3815],\n",
      "        [1.4184, 1.3865, 1.4158, 1.3597],\n",
      "        [1.4689, 1.4205, 1.3762, 1.3662]])\n",
      "layer_loss\n",
      " tensor([1.4241, 1.2340, 1.3092, 1.3092])\n",
      "tensor([[1.9602, 1.9065, 1.9784, 2.0414],\n",
      "        [1.7503, 1.6400, 1.6934, 1.7047],\n",
      "        [1.8570, 1.8153, 1.8536, 1.7801],\n",
      "        [1.9232, 1.8598, 1.8017, 1.7887]])\n",
      "head_score\n",
      " tensor([[1.0792e-04, 1.2268e-04, 1.1116e-04, 1.2113e-04],\n",
      "        [0.0000e+00, 1.3378e-04, 1.0909e-04, 1.1911e-04],\n",
      "        [0.0000e+00, 1.3358e-04, 1.0079e-04, 9.9686e-05],\n",
      "        [1.0761e-01, 9.3737e-02, 1.2829e-01, 1.0930e-01]])\n",
      "[(1, 0), (2, 0), (2, 2), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[5.4028e-05, 6.1898e-05, 5.6798e-05, 5.9136e-05],\n",
      "        [0.0000e+00, 8.0707e-05, 6.2788e-05, 6.9872e-05],\n",
      "        [0.0000e+00, 8.1092e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [5.9587e-02, 6.1166e-02, 7.9343e-02, 6.3517e-02]])\n",
      "head_loss\n",
      " tensor([[1.3157, 1.2838, 1.3276, 1.3749],\n",
      "        [1.3546, 1.2659, 1.3195, 1.3226],\n",
      "        [1.3546, 1.3172, 1.3546, 1.3546],\n",
      "        [1.4074, 1.3570, 1.3018, 1.3186]])\n",
      "layer_loss\n",
      " tensor([1.3655, 1.1820, 1.2389, 1.3188])\n",
      "tensor([[1.7965, 1.7530, 1.8128, 1.8773],\n",
      "        [1.6012, 1.4964, 1.5596, 1.5633],\n",
      "        [1.6782, 1.6319, 1.6782, 1.6782],\n",
      "        [1.8560, 1.7896, 1.7168, 1.7390]])\n",
      "head_score\n",
      " tensor([[9.7062e-05, 1.0851e-04, 1.0296e-04, 1.1102e-04],\n",
      "        [0.0000e+00, 1.2077e-04, 9.7926e-05, 1.0923e-04],\n",
      "        [0.0000e+00, 1.3233e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1059e-01, 1.0946e-01, 1.3621e-01, 1.1046e-01]])\n",
      "[(0, 0), (1, 0), (1, 2), (2, 0), (2, 2), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 6.7468e-05, 5.8358e-05, 6.3220e-05],\n",
      "        [0.0000e+00, 8.6015e-05, 0.0000e+00, 7.4260e-05],\n",
      "        [0.0000e+00, 7.9034e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [5.7457e-02, 6.5603e-02, 8.1551e-02, 6.8590e-02]])\n",
      "head_loss\n",
      " tensor([[1.2792, 1.2122, 1.2493, 1.2976],\n",
      "        [1.2792, 1.1845, 1.2792, 1.2569],\n",
      "        [1.2792, 1.2449, 1.2792, 1.2792],\n",
      "        [1.3259, 1.2913, 1.2271, 1.2501]])\n",
      "layer_loss\n",
      " tensor([1.3259, 1.1275, 1.1937, 1.2631])\n",
      "tensor([[1.6961, 1.6073, 1.6566, 1.7205],\n",
      "        [1.4423, 1.3355, 1.4423, 1.4171],\n",
      "        [1.5270, 1.4861, 1.5270, 1.5270],\n",
      "        [1.6748, 1.6310, 1.5499, 1.5789]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 1.0844e-04, 9.6673e-05, 1.0877e-04],\n",
      "        [0.0000e+00, 1.1487e-04, 0.0000e+00, 1.0524e-04],\n",
      "        [0.0000e+00, 1.1745e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [9.6226e-02, 1.0700e-01, 1.2639e-01, 1.0830e-01]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 2), (1, 3), (2, 0), (2, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af23dd8d8204323934a5ec6ebc603ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4236\n",
      "Precision: 0.6311, Recall: 0.5462, F1-Score: 0.5581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4136    0.6120    0.4936      2992\n",
      "           1     0.6786    0.4241    0.5220      2992\n",
      "           2     0.6361    0.6361    0.6361      3012\n",
      "           3     0.2936    0.6594    0.4063      2998\n",
      "           4     0.7448    0.7302    0.7374      2973\n",
      "           5     0.8999    0.4682    0.6160      3054\n",
      "           6     0.6330    0.4049    0.4939      3003\n",
      "           7     0.4639    0.6122    0.5278      3012\n",
      "           8     0.7050    0.4624    0.5585      2982\n",
      "           9     0.8427    0.4527    0.5890      2982\n",
      "\n",
      "    accuracy                         0.5461     30000\n",
      "   macro avg     0.6311    0.5462    0.5581     30000\n",
      "weighted avg     0.6313    0.5461    0.5580     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[4.9999e-05, 7.6041e-05, 4.8225e-05, 6.2064e-05],\n",
      "        [3.4291e-05, 6.8904e-05, 4.9083e-05, 5.1879e-05],\n",
      "        [4.2536e-05, 5.3444e-05, 4.6219e-05, 3.6453e-05],\n",
      "        [3.7141e-02, 6.8639e-02, 5.9368e-02, 6.8615e-02]])\n",
      "head_loss\n",
      " tensor([[1.3874, 1.4143, 1.3196, 1.2431],\n",
      "        [1.3138, 1.2790, 1.2957, 1.3500],\n",
      "        [1.2610, 1.2917, 1.3078, 1.2785],\n",
      "        [1.3050, 1.2487, 1.2913, 1.2684]])\n",
      "layer_loss\n",
      " tensor([1.3652, 1.3339, 1.2825, 1.2395])\n",
      "tensor([[1.8941, 1.9308, 1.8015, 1.6970],\n",
      "        [1.7525, 1.7061, 1.7283, 1.8009],\n",
      "        [1.6173, 1.6567, 1.6772, 1.6397],\n",
      "        [1.6176, 1.5478, 1.6006, 1.5723]])\n",
      "head_score\n",
      " tensor([[9.4702e-05, 1.4682e-04, 8.6878e-05, 1.0532e-04],\n",
      "        [6.0094e-05, 1.1756e-04, 8.4831e-05, 9.3428e-05],\n",
      "        [6.8792e-05, 8.8538e-05, 7.7520e-05, 5.9771e-05],\n",
      "        [6.0081e-02, 1.0624e-01, 9.5024e-02, 1.0788e-01]])\n",
      "[(1, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[4.7559e-05, 7.4521e-05, 4.9230e-05, 6.2515e-05],\n",
      "        [0.0000e+00, 7.0650e-05, 4.8986e-05, 5.3218e-05],\n",
      "        [4.3541e-05, 5.7317e-05, 5.0218e-05, 0.0000e+00],\n",
      "        [3.8255e-02, 6.6350e-02, 6.3511e-02, 6.9422e-02]])\n",
      "head_loss\n",
      " tensor([[1.3529, 1.3864, 1.2914, 1.2183],\n",
      "        [1.2811, 1.2473, 1.2680, 1.3243],\n",
      "        [1.2382, 1.2661, 1.2771, 1.2811],\n",
      "        [1.2816, 1.2298, 1.2669, 1.2412]])\n",
      "layer_loss\n",
      " tensor([1.3228, 1.3082, 1.2507, 1.2408])\n",
      "tensor([[1.7897, 1.8340, 1.7083, 1.6115],\n",
      "        [1.6759, 1.6316, 1.6587, 1.7325],\n",
      "        [1.5486, 1.5836, 1.5973, 1.6023],\n",
      "        [1.5902, 1.5259, 1.5719, 1.5402]])\n",
      "head_score\n",
      " tensor([[8.5117e-05, 1.3667e-04, 8.4099e-05, 1.0074e-04],\n",
      "        [0.0000e+00, 1.1527e-04, 8.1253e-05, 9.2197e-05],\n",
      "        [6.7429e-05, 9.0767e-05, 8.0214e-05, 0.0000e+00],\n",
      "        [6.0834e-02, 1.0125e-01, 9.9835e-02, 1.0692e-01]])\n",
      "[(1, 0), (2, 0), (2, 2), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[4.7055e-05, 7.3706e-05, 4.7023e-05, 5.7132e-05],\n",
      "        [0.0000e+00, 6.7824e-05, 4.8952e-05, 5.6362e-05],\n",
      "        [0.0000e+00, 6.4404e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [5.3437e-02, 6.3763e-02, 6.4048e-02, 8.4392e-02]])\n",
      "head_loss\n",
      " tensor([[1.3107, 1.3502, 1.2531, 1.1949],\n",
      "        [1.2334, 1.2051, 1.2243, 1.2924],\n",
      "        [1.2334, 1.2210, 1.2334, 1.2334],\n",
      "        [1.2562, 1.1906, 1.2133, 1.2074]])\n",
      "layer_loss\n",
      " tensor([1.3412, 1.2952, 1.2244, 1.2755])\n",
      "tensor([[1.7580, 1.8109, 1.6807, 1.6027],\n",
      "        [1.5975, 1.5609, 1.5858, 1.6739],\n",
      "        [1.5101, 1.4950, 1.5101, 1.5101],\n",
      "        [1.6022, 1.5186, 1.5475, 1.5400]])\n",
      "head_score\n",
      " tensor([[8.2721e-05, 1.3347e-04, 7.9033e-05, 9.1566e-05],\n",
      "        [0.0000e+00, 1.0586e-04, 7.7627e-05, 9.4345e-05],\n",
      "        [0.0000e+00, 9.6283e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [8.5616e-02, 9.6830e-02, 9.9114e-02, 1.2996e-01]])\n",
      "[(0, 2), (1, 0), (1, 2), (2, 0), (2, 2), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[5.3124e-05, 8.3596e-05, 0.0000e+00, 6.2865e-05],\n",
      "        [0.0000e+00, 7.5459e-05, 0.0000e+00, 6.2740e-05],\n",
      "        [0.0000e+00, 6.6623e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [6.5709e-02, 6.6767e-02, 6.2290e-02, 9.9015e-02]])\n",
      "head_loss\n",
      " tensor([[1.3460, 1.3902, 1.2516, 1.2290],\n",
      "        [1.2516, 1.2368, 1.2516, 1.3259],\n",
      "        [1.2516, 1.2474, 1.2516, 1.2516],\n",
      "        [1.2850, 1.2023, 1.2244, 1.2464]])\n",
      "layer_loss\n",
      " tensor([1.3887, 1.3580, 1.2244, 1.3707])\n",
      "tensor([[1.8692, 1.9306, 1.7380, 1.7067],\n",
      "        [1.6997, 1.6796, 1.6997, 1.8006],\n",
      "        [1.5324, 1.5273, 1.5324, 1.5324],\n",
      "        [1.7613, 1.6480, 1.6783, 1.7084]])\n",
      "head_score\n",
      " tensor([[9.9299e-05, 1.6139e-04, 0.0000e+00, 1.0729e-04],\n",
      "        [0.0000e+00, 1.2674e-04, 0.0000e+00, 1.1297e-04],\n",
      "        [0.0000e+00, 1.0175e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1573e-01, 1.1003e-01, 1.0454e-01, 1.6916e-01]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1), (2, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5382cf6bb7c4401f8fef638a016a8be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3959\n",
      "Precision: 0.6355, Recall: 0.5600, F1-Score: 0.5739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4245    0.6110    0.5010      2992\n",
      "           1     0.6673    0.4532    0.5398      2992\n",
      "           2     0.6522    0.6325    0.6422      3012\n",
      "           3     0.2969    0.6631    0.4102      2998\n",
      "           4     0.7855    0.6700    0.7232      2973\n",
      "           5     0.9136    0.4918    0.6394      3054\n",
      "           6     0.6416    0.4029    0.4950      3003\n",
      "           7     0.5000    0.5933    0.5427      3012\n",
      "           8     0.6825    0.5429    0.6048      2982\n",
      "           9     0.7906    0.5392    0.6411      2982\n",
      "\n",
      "    accuracy                         0.5598     30000\n",
      "   macro avg     0.6355    0.5600    0.5739     30000\n",
      "weighted avg     0.6357    0.5598    0.5739     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[4.0902e-05, 5.5916e-05, 5.2655e-05, 6.8180e-05],\n",
      "        [4.5500e-05, 1.2641e-04, 6.4290e-05, 5.4530e-05],\n",
      "        [5.1687e-05, 5.6569e-05, 5.4128e-05, 4.2799e-05],\n",
      "        [6.1029e-02, 4.7138e-02, 5.8882e-02, 7.6078e-02]])\n",
      "head_loss\n",
      " tensor([[1.1745, 1.1519, 1.1427, 1.2256],\n",
      "        [1.1498, 1.2636, 1.1328, 1.1580],\n",
      "        [1.1927, 1.1267, 1.1643, 1.1345],\n",
      "        [1.1425, 1.1781, 1.1257, 1.1298]])\n",
      "layer_loss\n",
      " tensor([1.1994, 1.2550, 1.1344, 1.1978])\n",
      "tensor([[1.4087, 1.3817, 1.3705, 1.4700],\n",
      "        [1.4430, 1.5858, 1.4216, 1.4532],\n",
      "        [1.3530, 1.2782, 1.3208, 1.2870],\n",
      "        [1.3685, 1.4111, 1.3484, 1.3533]])\n",
      "head_score\n",
      " tensor([[5.7620e-05, 7.7259e-05, 7.2166e-05, 1.0023e-04],\n",
      "        [6.5657e-05, 2.0045e-04, 9.1396e-05, 7.9243e-05],\n",
      "        [6.9935e-05, 7.2307e-05, 7.1494e-05, 5.5083e-05],\n",
      "        [8.3520e-02, 6.6516e-02, 7.9396e-02, 1.0295e-01]])\n",
      "[(0, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 6.3150e-05, 5.8019e-05, 7.4549e-05],\n",
      "        [4.7456e-05, 1.2658e-04, 6.5775e-05, 5.1523e-05],\n",
      "        [5.5054e-05, 5.8837e-05, 5.6931e-05, 0.0000e+00],\n",
      "        [6.8827e-02, 4.7584e-02, 6.6049e-02, 8.3226e-02]])\n",
      "head_loss\n",
      " tensor([[1.1525, 1.1531, 1.1441, 1.2322],\n",
      "        [1.1442, 1.2695, 1.1301, 1.1516],\n",
      "        [1.1941, 1.1300, 1.1637, 1.1525],\n",
      "        [1.1488, 1.1713, 1.1250, 1.1307]])\n",
      "layer_loss\n",
      " tensor([1.1802, 1.2599, 1.1340, 1.2366])\n",
      "tensor([[1.3601, 1.3608, 1.3502, 1.4542],\n",
      "        [1.4416, 1.5994, 1.4238, 1.4508],\n",
      "        [1.3541, 1.2815, 1.3197, 1.3069],\n",
      "        [1.4207, 1.4485, 1.3911, 1.3982]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 8.5936e-05, 7.8341e-05, 1.0841e-04],\n",
      "        [6.8411e-05, 2.0245e-04, 9.3649e-05, 7.4748e-05],\n",
      "        [7.4547e-05, 7.5398e-05, 7.5130e-05, 0.0000e+00],\n",
      "        [9.7780e-02, 6.8925e-02, 9.1884e-02, 1.1637e-01]])\n",
      "[(0, 0), (1, 0), (2, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 6.5526e-05, 5.9935e-05, 7.6117e-05],\n",
      "        [0.0000e+00, 1.3106e-04, 6.6259e-05, 5.1384e-05],\n",
      "        [0.0000e+00, 6.2742e-05, 6.2580e-05, 0.0000e+00],\n",
      "        [7.6085e-02, 4.8902e-02, 6.8882e-02, 8.7578e-02]])\n",
      "head_loss\n",
      " tensor([[1.1849, 1.1859, 1.1757, 1.2654],\n",
      "        [1.1849, 1.3067, 1.1648, 1.1800],\n",
      "        [1.1849, 1.1712, 1.2007, 1.1849],\n",
      "        [1.1802, 1.1987, 1.1579, 1.1744]])\n",
      "layer_loss\n",
      " tensor([1.1802, 1.3016, 1.1756, 1.2757])\n",
      "tensor([[1.3984, 1.3996, 1.3875, 1.4933],\n",
      "        [1.5423, 1.7007, 1.5161, 1.5359],\n",
      "        [1.3930, 1.3769, 1.4115, 1.3930],\n",
      "        [1.5055, 1.5291, 1.4771, 1.4981]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 9.1709e-05, 8.3161e-05, 1.1367e-04],\n",
      "        [0.0000e+00, 2.2291e-04, 1.0045e-04, 7.8920e-05],\n",
      "        [0.0000e+00, 8.6390e-05, 8.8333e-05, 0.0000e+00],\n",
      "        [1.1455e-01, 7.4778e-02, 1.0174e-01, 1.3120e-01]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 3), (2, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 7.4780e-05, 0.0000e+00, 8.4268e-05],\n",
      "        [0.0000e+00, 1.3899e-04, 6.8994e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 6.2808e-05, 7.2439e-05, 0.0000e+00],\n",
      "        [8.3991e-02, 4.7778e-02, 7.3632e-02, 9.2611e-02]])\n",
      "head_loss\n",
      " tensor([[1.1725, 1.1813, 1.1725, 1.2636],\n",
      "        [1.1725, 1.3234, 1.1568, 1.1725],\n",
      "        [1.1725, 1.1665, 1.1973, 1.1725],\n",
      "        [1.1901, 1.1793, 1.1535, 1.1920]])\n",
      "layer_loss\n",
      " tensor([1.1901, 1.3438, 1.1931, 1.3078])\n",
      "tensor([[1.3954, 1.4059, 1.3954, 1.5038],\n",
      "        [1.5756, 1.7784, 1.5544, 1.5756],\n",
      "        [1.3989, 1.3916, 1.4285, 1.3989],\n",
      "        [1.5564, 1.5423, 1.5086, 1.5589]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 1.0513e-04, 0.0000e+00, 1.2673e-04],\n",
      "        [0.0000e+00, 2.4718e-04, 1.0725e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 8.7406e-05, 1.0348e-04, 0.0000e+00],\n",
      "        [1.3072e-01, 7.3687e-02, 1.1108e-01, 1.4437e-01]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae2e715539e42a2a9bdfa954a88f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3753\n",
      "Precision: 0.6363, Recall: 0.5709, F1-Score: 0.5841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4276    0.6029    0.5003      2992\n",
      "           1     0.6797    0.4412    0.5351      2992\n",
      "           2     0.6489    0.6338    0.6412      3012\n",
      "           3     0.3040    0.6624    0.4168      2998\n",
      "           4     0.7667    0.6932    0.7281      2973\n",
      "           5     0.8861    0.6343    0.7393      3054\n",
      "           6     0.6333    0.4106    0.4982      3003\n",
      "           7     0.5291    0.5876    0.5569      3012\n",
      "           8     0.6917    0.5117    0.5883      2982\n",
      "           9     0.7953    0.5315    0.6372      2982\n",
      "\n",
      "    accuracy                         0.5710     30000\n",
      "   macro avg     0.6363    0.5709    0.5841     30000\n",
      "weighted avg     0.6365    0.5710    0.5843     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[3.9856e-05, 5.2533e-05, 3.7867e-05, 4.3479e-05],\n",
      "        [4.2380e-05, 5.1160e-05, 4.3393e-05, 3.7194e-05],\n",
      "        [4.0360e-05, 7.2976e-05, 5.0401e-05, 3.8193e-05],\n",
      "        [4.1402e-02, 5.5559e-02, 4.6915e-02, 4.9429e-02]])\n",
      "head_loss\n",
      " tensor([[0.9726, 1.0568, 0.9754, 0.9352],\n",
      "        [1.0081, 0.9829, 0.9644, 0.9789],\n",
      "        [0.9474, 0.9600, 0.9805, 0.9782],\n",
      "        [0.9802, 1.0357, 1.0243, 1.0230]])\n",
      "layer_loss\n",
      " tensor([0.9604, 1.1061, 1.0069, 0.9556])\n",
      "tensor([[0.9341, 1.0149, 0.9368, 0.8981],\n",
      "        [1.1150, 1.0872, 1.0667, 1.0828],\n",
      "        [0.9540, 0.9667, 0.9873, 0.9850],\n",
      "        [0.9368, 0.9897, 0.9789, 0.9776]])\n",
      "head_score\n",
      " tensor([[3.7228e-05, 5.3315e-05, 3.5473e-05, 3.9049e-05],\n",
      "        [4.7255e-05, 5.5619e-05, 4.6288e-05, 4.0272e-05],\n",
      "        [3.8503e-05, 7.0542e-05, 4.9762e-05, 3.7620e-05],\n",
      "        [3.8784e-02, 5.4987e-02, 4.5924e-02, 4.8320e-02]])\n",
      "[(0, 0), (0, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 5.8909e-05, 0.0000e+00, 4.8556e-05],\n",
      "        [4.2107e-05, 5.3078e-05, 4.5327e-05, 3.8821e-05],\n",
      "        [4.3603e-05, 7.0994e-05, 5.2922e-05, 3.9282e-05],\n",
      "        [4.5405e-02, 5.7307e-02, 5.0045e-02, 4.6959e-02]])\n",
      "head_loss\n",
      " tensor([[0.9661, 1.0520, 0.9661, 0.9140],\n",
      "        [0.9932, 0.9720, 0.9527, 0.9696],\n",
      "        [0.9275, 0.9465, 0.9641, 0.9605],\n",
      "        [0.9590, 1.0111, 1.0121, 0.9916]])\n",
      "layer_loss\n",
      " tensor([0.9517, 1.1139, 1.0107, 0.9303])\n",
      "tensor([[0.9194, 1.0012, 0.9194, 0.8699],\n",
      "        [1.1063, 1.0827, 1.0612, 1.0800],\n",
      "        [0.9374, 0.9565, 0.9744, 0.9707],\n",
      "        [0.8922, 0.9406, 0.9416, 0.9225]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 5.8980e-05, 0.0000e+00, 4.2237e-05],\n",
      "        [4.6585e-05, 5.7467e-05, 4.8100e-05, 4.1928e-05],\n",
      "        [4.0875e-05, 6.7908e-05, 5.1567e-05, 3.8131e-05],\n",
      "        [4.0509e-02, 5.3905e-02, 4.7122e-02, 4.3318e-02]])\n",
      "[(0, 0), (0, 2), (2, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 6.1960e-05, 0.0000e+00, 5.0788e-05],\n",
      "        [4.5787e-05, 5.3161e-05, 4.6716e-05, 4.0544e-05],\n",
      "        [0.0000e+00, 7.7291e-05, 5.7277e-05, 0.0000e+00],\n",
      "        [5.1248e-02, 7.1783e-02, 6.1605e-02, 5.2407e-02]])\n",
      "head_loss\n",
      " tensor([[0.9220, 1.0115, 0.9220, 0.8689],\n",
      "        [0.9541, 0.9347, 0.9096, 0.9266],\n",
      "        [0.9220, 0.9055, 0.9180, 0.9220],\n",
      "        [0.9232, 0.9887, 0.9897, 0.9611]])\n",
      "layer_loss\n",
      " tensor([0.9556, 1.1102, 0.9884, 0.8938])\n",
      "tensor([[0.8810, 0.9666, 0.8810, 0.8303],\n",
      "        [1.0592, 1.0377, 1.0098, 1.0287],\n",
      "        [0.9113, 0.8950, 0.9074, 0.9113],\n",
      "        [0.8252, 0.8837, 0.8846, 0.8591]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 5.9888e-05, 0.0000e+00, 4.2168e-05],\n",
      "        [4.8500e-05, 5.5163e-05, 4.7175e-05, 4.1707e-05],\n",
      "        [0.0000e+00, 6.9177e-05, 5.1973e-05, 0.0000e+00],\n",
      "        [4.2291e-02, 6.3437e-02, 5.4499e-02, 4.5021e-02]])\n",
      "[(0, 0), (0, 2), (0, 3), (1, 3), (2, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 6.7236e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [4.6887e-05, 5.3391e-05, 5.0373e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 7.1156e-05, 5.4726e-05, 0.0000e+00],\n",
      "        [5.2945e-02, 7.4510e-02, 6.5564e-02, 4.9487e-02]])\n",
      "head_loss\n",
      " tensor([[0.8748, 0.9720, 0.8748, 0.8748],\n",
      "        [0.9082, 0.8935, 0.8742, 0.8748],\n",
      "        [0.8748, 0.8642, 0.8716, 0.8748],\n",
      "        [0.8800, 0.9415, 0.9465, 0.8938]])\n",
      "layer_loss\n",
      " tensor([0.9140, 1.0899, 0.9607, 0.8938])\n",
      "tensor([[0.7995, 0.8884, 0.7995, 0.7995],\n",
      "        [0.9899, 0.9739, 0.9528, 0.9535],\n",
      "        [0.8404, 0.8302, 0.8374, 0.8404],\n",
      "        [0.7866, 0.8416, 0.8460, 0.7990]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 5.9731e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [4.6415e-05, 5.1998e-05, 4.7998e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 5.9076e-05, 4.5827e-05, 0.0000e+00],\n",
      "        [4.1646e-02, 6.2707e-02, 5.5467e-02, 3.9538e-02]])\n",
      "[(0, 0), (0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b188e9e57a334f51b53fdb9555f27b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7314f88f8900>Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/jieungkim/anaconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7314f88f8900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/jieungkim/anaconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4013\n",
      "Precision: 0.6347, Recall: 0.5566, F1-Score: 0.5726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4434    0.5775    0.5017      2992\n",
      "           1     0.6404    0.4131    0.5022      2992\n",
      "           2     0.6568    0.5979    0.6260      3012\n",
      "           3     0.2771    0.6955    0.3963      2998\n",
      "           4     0.7593    0.6895    0.7227      2973\n",
      "           5     0.8773    0.6113    0.7206      3054\n",
      "           6     0.6293    0.4099    0.4965      3003\n",
      "           7     0.5424    0.5837    0.5623      3012\n",
      "           8     0.7074    0.4752    0.5685      2982\n",
      "           9     0.8136    0.5124    0.6288      2982\n",
      "\n",
      "    accuracy                         0.5567     30000\n",
      "   macro avg     0.6347    0.5566    0.5726     30000\n",
      "weighted avg     0.6349    0.5567    0.5727     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[3.3910e-05, 5.0382e-05, 6.7213e-05, 5.9223e-05],\n",
      "        [3.3663e-05, 6.3479e-05, 4.5566e-05, 3.2855e-05],\n",
      "        [2.6289e-05, 5.0184e-05, 3.8089e-05, 3.9486e-05],\n",
      "        [5.7372e-02, 3.9826e-02, 3.9364e-02, 5.6368e-02]])\n",
      "head_loss\n",
      " tensor([[0.9056, 0.9962, 1.0475, 0.9986],\n",
      "        [0.9231, 0.9938, 0.9666, 0.9119],\n",
      "        [0.9236, 0.9957, 0.9624, 0.9440],\n",
      "        [0.9458, 0.9123, 0.9100, 0.9725]])\n",
      "layer_loss\n",
      " tensor([0.9269, 1.1319, 1.1186, 1.0567])\n",
      "tensor([[0.8394, 0.9234, 0.9710, 0.9257],\n",
      "        [1.0449, 1.1249, 1.0941, 1.0322],\n",
      "        [1.0331, 1.1138, 1.0764, 1.0559],\n",
      "        [0.9994, 0.9640, 0.9616, 1.0276]])\n",
      "head_score\n",
      " tensor([[2.8464e-05, 4.6522e-05, 6.5262e-05, 5.4821e-05],\n",
      "        [3.5174e-05, 7.1405e-05, 4.9853e-05, 3.3912e-05],\n",
      "        [2.7159e-05, 5.5893e-05, 4.1000e-05, 4.1692e-05],\n",
      "        [5.7338e-02, 3.8391e-02, 3.7853e-02, 5.7925e-02]])\n",
      "[(0, 0), (2, 0)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 5.1436e-05, 6.7270e-05, 6.1624e-05],\n",
      "        [3.3388e-05, 6.4170e-05, 4.6276e-05, 3.2921e-05],\n",
      "        [0.0000e+00, 5.3817e-05, 3.8740e-05, 4.3613e-05],\n",
      "        [6.4606e-02, 4.3248e-02, 3.8026e-02, 5.8398e-02]])\n",
      "head_loss\n",
      " tensor([[0.9041, 0.9730, 1.0247, 0.9791],\n",
      "        [0.9006, 0.9674, 0.9371, 0.8889],\n",
      "        [0.9041, 0.9833, 0.9376, 0.9375],\n",
      "        [0.9296, 0.8816, 0.8849, 0.9427]])\n",
      "layer_loss\n",
      " tensor([0.9269, 1.1064, 1.0826, 1.0516])\n",
      "tensor([[0.8381, 0.9019, 0.9498, 0.9076],\n",
      "        [0.9964, 1.0703, 1.0369, 0.9835],\n",
      "        [0.9788, 1.0646, 1.0151, 1.0150],\n",
      "        [0.9776, 0.9271, 0.9305, 0.9914]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 4.6391e-05, 6.3896e-05, 5.5929e-05],\n",
      "        [3.3269e-05, 6.8683e-05, 4.7982e-05, 3.2379e-05],\n",
      "        [0.0000e+00, 5.7293e-05, 3.9323e-05, 4.4266e-05],\n",
      "        [6.3156e-02, 4.0097e-02, 3.5384e-02, 5.7894e-02]])\n",
      "[(0, 0), (1, 0), (1, 3), (2, 0)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 5.0184e-05, 6.6999e-05, 6.1126e-05],\n",
      "        [0.0000e+00, 6.6062e-05, 4.6974e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 5.4209e-05, 4.1301e-05, 4.8284e-05],\n",
      "        [6.6772e-02, 4.2696e-02, 3.5636e-02, 5.6836e-02]])\n",
      "head_loss\n",
      " tensor([[0.8841, 0.9455, 1.0008, 0.9553],\n",
      "        [0.8841, 0.9512, 0.9223, 0.8841],\n",
      "        [0.8841, 0.9623, 0.9108, 0.9265],\n",
      "        [0.9004, 0.8602, 0.8630, 0.9184]])\n",
      "layer_loss\n",
      " tensor([0.9004, 1.0778, 1.0537, 1.0616])\n",
      "tensor([[0.7960, 0.8513, 0.9010, 0.8601],\n",
      "        [0.9528, 1.0252, 0.9941, 0.9528],\n",
      "        [0.9315, 1.0140, 0.9597, 0.9762],\n",
      "        [0.9558, 0.9132, 0.9161, 0.9749]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 4.2722e-05, 6.0369e-05, 5.2577e-05],\n",
      "        [0.0000e+00, 6.7724e-05, 4.6697e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 5.4967e-05, 3.9636e-05, 4.7136e-05],\n",
      "        [6.3821e-02, 3.8989e-02, 3.2646e-02, 5.5412e-02]])\n",
      "[(0, 0), (0, 1), (1, 0), (1, 3), (2, 0), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 0.0000e+00, 7.7418e-05, 7.0190e-05],\n",
      "        [0.0000e+00, 7.0748e-05, 5.0058e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 6.3862e-05, 0.0000e+00, 5.8240e-05],\n",
      "        [8.7803e-02, 4.8813e-02, 2.9583e-02, 6.6426e-02]])\n",
      "head_loss\n",
      " tensor([[0.9692, 0.9692, 1.1045, 1.0487],\n",
      "        [0.9692, 1.0426, 1.0141, 0.9692],\n",
      "        [0.9692, 1.0712, 0.9692, 1.0320],\n",
      "        [1.0022, 0.9400, 0.9635, 1.0023]])\n",
      "layer_loss\n",
      " tensor([1.0022, 1.1077, 1.1597, 1.1804])\n",
      "tensor([[0.9713, 0.9713, 1.1069, 1.0511],\n",
      "        [1.0735, 1.1549, 1.1233, 1.0735],\n",
      "        [1.1239, 1.2422, 1.1239, 1.1968],\n",
      "        [1.1830, 1.1096, 1.1374, 1.1831]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 0.0000e+00, 8.5695e-05, 7.3774e-05],\n",
      "        [0.0000e+00, 8.1708e-05, 5.6231e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 7.9329e-05, 0.0000e+00, 6.9702e-05],\n",
      "        [1.0387e-01, 5.4163e-02, 3.3647e-02, 7.8590e-02]])\n",
      "[(0, 0), (0, 1), (1, 0), (1, 2), (1, 3), (2, 0), (2, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fe29013d2548378644e115f193d97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4471\n",
      "Precision: 0.6199, Recall: 0.5417, F1-Score: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3909    0.6270    0.4816      2992\n",
      "           1     0.7084    0.3937    0.5061      2992\n",
      "           2     0.6418    0.6371    0.6395      3012\n",
      "           3     0.3060    0.6341    0.4128      2998\n",
      "           4     0.7063    0.7659    0.7349      2973\n",
      "           5     0.8684    0.4126    0.5594      3054\n",
      "           6     0.6237    0.4056    0.4915      3003\n",
      "           7     0.4542    0.5591    0.5012      3012\n",
      "           8     0.6671    0.5443    0.5994      2982\n",
      "           9     0.8327    0.4373    0.5734      2982\n",
      "\n",
      "    accuracy                         0.5413     30000\n",
      "   macro avg     0.6199    0.5417    0.5500     30000\n",
      "weighted avg     0.6201    0.5413    0.5498     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[3.9889e-05, 5.6511e-05, 3.9480e-05, 4.9755e-05],\n",
      "        [5.3199e-05, 4.3852e-05, 5.0532e-05, 4.2366e-05],\n",
      "        [3.5718e-05, 2.8492e-05, 2.4804e-05, 4.8345e-05],\n",
      "        [4.1895e-02, 3.9775e-02, 5.0310e-02, 4.5638e-02]])\n",
      "head_loss\n",
      " tensor([[0.7846, 0.7523, 0.7913, 0.7691],\n",
      "        [0.8101, 0.7565, 0.7867, 0.7508],\n",
      "        [0.7885, 0.7732, 0.7829, 0.7575],\n",
      "        [0.7362, 0.7439, 0.7616, 0.7519]])\n",
      "layer_loss\n",
      " tensor([0.8447, 0.7249, 0.8177, 0.7240])\n",
      "tensor([[0.6628, 0.6354, 0.6684, 0.6497],\n",
      "        [0.5873, 0.5484, 0.5703, 0.5442],\n",
      "        [0.6448, 0.6323, 0.6402, 0.6194],\n",
      "        [0.5330, 0.5385, 0.5514, 0.5444]])\n",
      "head_score\n",
      " tensor([[2.6437e-05, 3.5909e-05, 2.6387e-05, 3.2324e-05],\n",
      "        [3.1242e-05, 2.4049e-05, 2.8819e-05, 2.3057e-05],\n",
      "        [2.3031e-05, 1.8014e-05, 1.5878e-05, 2.9947e-05],\n",
      "        [2.2329e-02, 2.1421e-02, 2.7739e-02, 2.4844e-02]])\n",
      "[(2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[4.0479e-05, 5.7775e-05, 3.9871e-05, 5.1156e-05],\n",
      "        [5.4182e-05, 4.1153e-05, 5.2370e-05, 4.4472e-05],\n",
      "        [4.0677e-05, 0.0000e+00, 0.0000e+00, 5.2812e-05],\n",
      "        [4.8070e-02, 4.2614e-02, 5.3063e-02, 4.5411e-02]])\n",
      "head_loss\n",
      " tensor([[0.8116, 0.7726, 0.8182, 0.8019],\n",
      "        [0.8322, 0.7806, 0.8179, 0.7789],\n",
      "        [0.8222, 0.7927, 0.7927, 0.7748],\n",
      "        [0.7564, 0.7746, 0.7828, 0.7745]])\n",
      "layer_loss\n",
      " tensor([0.8759, 0.7406, 0.8307, 0.7468])\n",
      "tensor([[0.7109, 0.6768, 0.7167, 0.7024],\n",
      "        [0.6164, 0.5782, 0.6058, 0.5769],\n",
      "        [0.6830, 0.6585, 0.6585, 0.6436],\n",
      "        [0.5648, 0.5785, 0.5845, 0.5784]])\n",
      "head_score\n",
      " tensor([[2.8776e-05, 3.9101e-05, 2.8577e-05, 3.5933e-05],\n",
      "        [3.3395e-05, 2.3793e-05, 3.1724e-05, 2.5654e-05],\n",
      "        [2.7782e-05, 0.0000e+00, 0.0000e+00, 3.3991e-05],\n",
      "        [2.7152e-02, 2.4650e-02, 3.1018e-02, 2.6265e-02]])\n",
      "[(1, 1), (1, 3), (2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[4.1115e-05, 5.5215e-05, 3.8672e-05, 5.0198e-05],\n",
      "        [5.6037e-05, 0.0000e+00, 6.2745e-05, 0.0000e+00],\n",
      "        [4.2340e-05, 0.0000e+00, 0.0000e+00, 5.3131e-05],\n",
      "        [5.2971e-02, 4.3801e-02, 4.9393e-02, 3.8815e-02]])\n",
      "head_loss\n",
      " tensor([[0.7931, 0.7462, 0.7932, 0.7817],\n",
      "        [0.8022, 0.7659, 0.7988, 0.7659],\n",
      "        [0.7931, 0.7659, 0.7659, 0.7439],\n",
      "        [0.7245, 0.7495, 0.7531, 0.7519]])\n",
      "layer_loss\n",
      " tensor([0.8491, 0.7280, 0.8115, 0.7274])\n",
      "tensor([[0.6734, 0.6336, 0.6735, 0.6638],\n",
      "        [0.5840, 0.5576, 0.5815, 0.5576],\n",
      "        [0.6436, 0.6216, 0.6216, 0.6037],\n",
      "        [0.5270, 0.5452, 0.5478, 0.5469]])\n",
      "head_score\n",
      " tensor([[2.7688e-05, 3.4985e-05, 2.6046e-05, 3.3322e-05],\n",
      "        [3.2728e-05, 0.0000e+00, 3.6489e-05, 0.0000e+00],\n",
      "        [2.7252e-05, 0.0000e+00, 0.0000e+00, 3.2076e-05],\n",
      "        [2.7916e-02, 2.3879e-02, 2.7058e-02, 2.1230e-02]])\n",
      "[(0, 2), (1, 1), (1, 3), (2, 0), (2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[4.6665e-05, 6.5112e-05, 0.0000e+00, 6.0399e-05],\n",
      "        [7.0115e-05, 0.0000e+00, 7.2228e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7985e-05],\n",
      "        [6.5913e-02, 5.4513e-02, 5.6703e-02, 4.7328e-02]])\n",
      "head_loss\n",
      " tensor([[0.8687, 0.8113, 0.8214, 0.8562],\n",
      "        [0.9006, 0.8214, 0.8777, 0.8214],\n",
      "        [0.8214, 0.8214, 0.8214, 0.8203],\n",
      "        [0.7677, 0.8007, 0.7989, 0.7982]])\n",
      "layer_loss\n",
      " tensor([0.8928, 0.7869, 0.8481, 0.8136])\n",
      "tensor([[0.7756, 0.7243, 0.7333, 0.7645],\n",
      "        [0.7087, 0.6463, 0.6907, 0.6463],\n",
      "        [0.6966, 0.6966, 0.6966, 0.6957],\n",
      "        [0.6246, 0.6514, 0.6500, 0.6494]])\n",
      "head_score\n",
      " tensor([[3.6194e-05, 4.7162e-05, 0.0000e+00, 4.6172e-05],\n",
      "        [4.9692e-05, 0.0000e+00, 4.9889e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7294e-05],\n",
      "        [4.1170e-02, 3.5511e-02, 3.6857e-02, 3.0735e-02]])\n",
      "[(0, 0), (0, 2), (0, 3), (1, 1), (1, 3), (2, 0), (2, 1), (2, 2)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730ec2941ee948a2b7a924b10ef0c8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4166\n",
      "Precision: 0.6279, Recall: 0.5461, F1-Score: 0.5572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4224    0.5648    0.4833      2992\n",
      "           1     0.6486    0.3690    0.4704      2992\n",
      "           2     0.6829    0.5292    0.5963      3012\n",
      "           3     0.2739    0.7041    0.3944      2998\n",
      "           4     0.7942    0.6424    0.7103      2973\n",
      "           5     0.8422    0.7377    0.7865      3054\n",
      "           6     0.6392    0.4006    0.4925      3003\n",
      "           7     0.5145    0.6524    0.5753      3012\n",
      "           8     0.7013    0.2921    0.4124      2982\n",
      "           9     0.7595    0.5687    0.6504      2982\n",
      "\n",
      "    accuracy                         0.5466     30000\n",
      "   macro avg     0.6279    0.5461    0.5572     30000\n",
      "weighted avg     0.6281    0.5466    0.5576     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[7.1710e-05, 6.3561e-05, 6.9254e-05, 7.1243e-05],\n",
      "        [5.1929e-05, 8.9838e-05, 7.5928e-05, 9.6699e-05],\n",
      "        [6.0020e-05, 8.6839e-05, 5.4598e-05, 6.8516e-05],\n",
      "        [8.9570e-02, 1.2541e-01, 6.4886e-02, 9.6133e-02]])\n",
      "head_loss\n",
      " tensor([[2.3838, 2.3777, 2.3516, 2.3571],\n",
      "        [2.2915, 2.3145, 2.3668, 2.3158],\n",
      "        [2.2746, 2.3747, 2.3109, 2.4011],\n",
      "        [2.4057, 2.2232, 2.3131, 2.3992]])\n",
      "layer_loss\n",
      " tensor([2.3621, 2.3263, 2.3709, 2.5027])\n",
      "tensor([[5.6306, 5.6164, 5.5546, 5.5676],\n",
      "        [5.3307, 5.3841, 5.5057, 5.3873],\n",
      "        [5.3930, 5.6303, 5.4789, 5.6929],\n",
      "        [6.0207, 5.5641, 5.7890, 6.0045]])\n",
      "head_score\n",
      " tensor([[4.0377e-04, 3.5698e-04, 3.8467e-04, 3.9665e-04],\n",
      "        [2.7682e-04, 4.8370e-04, 4.1804e-04, 5.2094e-04],\n",
      "        [3.2368e-04, 4.8893e-04, 2.9914e-04, 3.9006e-04],\n",
      "        [5.3928e-01, 6.9779e-01, 3.7562e-01, 5.7723e-01]])\n",
      "[(1, 0), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[6.7995e-05, 6.2996e-05, 6.9669e-05, 6.9889e-05],\n",
      "        [0.0000e+00, 8.9391e-05, 7.7252e-05, 9.9866e-05],\n",
      "        [6.2616e-05, 9.0183e-05, 0.0000e+00, 7.3227e-05],\n",
      "        [9.1267e-02, 1.2421e-01, 6.3107e-02, 9.3797e-02]])\n",
      "head_loss\n",
      " tensor([[2.3354, 2.3416, 2.3119, 2.3121],\n",
      "        [2.2813, 2.2735, 2.3291, 2.2689],\n",
      "        [2.2339, 2.3278, 2.2813, 2.3627],\n",
      "        [2.3639, 2.1860, 2.2722, 2.3509]])\n",
      "layer_loss\n",
      " tensor([2.3466, 2.2777, 2.3449, 2.4428])\n",
      "tensor([[5.4804, 5.4950, 5.4252, 5.4257],\n",
      "        [5.1961, 5.1783, 5.3048, 5.1679],\n",
      "        [5.2381, 5.4584, 5.3494, 5.5401],\n",
      "        [5.7746, 5.3400, 5.5507, 5.7427]])\n",
      "head_score\n",
      " tensor([[3.7264e-04, 3.4616e-04, 3.7797e-04, 3.7920e-04],\n",
      "        [0.0000e+00, 4.6289e-04, 4.0981e-04, 5.1610e-04],\n",
      "        [3.2799e-04, 4.9226e-04, 0.0000e+00, 4.0568e-04],\n",
      "        [5.2703e-01, 6.6326e-01, 3.5029e-01, 5.3865e-01]])\n",
      "[(0, 1), (1, 0), (2, 0), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[6.8347e-05, 0.0000e+00, 7.1774e-05, 7.3337e-05],\n",
      "        [0.0000e+00, 9.0992e-05, 7.9640e-05, 1.0022e-04],\n",
      "        [0.0000e+00, 9.6050e-05, 0.0000e+00, 7.8099e-05],\n",
      "        [9.8045e-02, 1.3213e-01, 6.9085e-02, 8.9683e-02]])\n",
      "head_loss\n",
      " tensor([[2.3481, 2.2953, 2.3291, 2.3275],\n",
      "        [2.2953, 2.2950, 2.3513, 2.2716],\n",
      "        [2.2953, 2.3364, 2.2953, 2.3857],\n",
      "        [2.3706, 2.2096, 2.2815, 2.3490]])\n",
      "layer_loss\n",
      " tensor([2.4088, 2.2306, 2.3608, 2.4320])\n",
      "tensor([[5.6563, 5.5290, 5.6103, 5.6065],\n",
      "        [5.1198, 5.1191, 5.2447, 5.0670],\n",
      "        [5.4187, 5.5157, 5.4187, 5.6321],\n",
      "        [5.7653, 5.3737, 5.5486, 5.7128]])\n",
      "head_score\n",
      " tensor([[3.8659e-04, 0.0000e+00, 4.0267e-04, 4.1116e-04],\n",
      "        [0.0000e+00, 4.6580e-04, 4.1769e-04, 5.0783e-04],\n",
      "        [0.0000e+00, 5.2978e-04, 0.0000e+00, 4.3986e-04],\n",
      "        [5.6525e-01, 7.1003e-01, 3.8332e-01, 5.1234e-01]])\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (2, 0), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7942e-05],\n",
      "        [0.0000e+00, 9.8619e-05, 9.0227e-05, 1.1266e-04],\n",
      "        [0.0000e+00, 9.6602e-05, 0.0000e+00, 8.7955e-05],\n",
      "        [1.1003e-01, 1.6360e-01, 9.0691e-02, 9.0541e-02]])\n",
      "head_loss\n",
      " tensor([[2.3901, 2.3901, 2.3901, 2.4377],\n",
      "        [2.3901, 2.4008, 2.4604, 2.3560],\n",
      "        [2.3901, 2.4299, 2.3901, 2.4908],\n",
      "        [2.4454, 2.2977, 2.3727, 2.4288]])\n",
      "layer_loss\n",
      " tensor([2.4454, 2.3374, 2.4274, 2.5261])\n",
      "tensor([[5.8447, 5.8447, 5.8447, 5.9611],\n",
      "        [5.5865, 5.6116, 5.7507, 5.5067],\n",
      "        [5.8018, 5.8984, 5.8018, 6.0461],\n",
      "        [6.1772, 5.8042, 5.9935, 6.1352]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2423e-04],\n",
      "        [0.0000e+00, 5.5341e-04, 5.1887e-04, 6.2040e-04],\n",
      "        [0.0000e+00, 5.6980e-04, 0.0000e+00, 5.3178e-04],\n",
      "        [6.7968e-01, 9.4956e-01, 5.4356e-01, 5.5549e-01]])\n",
      "[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (2, 0), (2, 2)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb77f78942804beabb25f8c51ea5992b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4292\n",
      "Precision: 0.6419, Recall: 0.5539, F1-Score: 0.5692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4178    0.6009    0.4929      2992\n",
      "           1     0.7021    0.3443    0.4620      2992\n",
      "           2     0.6712    0.5930    0.6296      3012\n",
      "           3     0.2799    0.6791    0.3965      2998\n",
      "           4     0.7853    0.6583    0.7162      2973\n",
      "           5     0.9235    0.5537    0.6923      3054\n",
      "           6     0.6356    0.4129    0.5006      3003\n",
      "           7     0.5169    0.6365    0.5705      3012\n",
      "           8     0.6723    0.5567    0.6091      2982\n",
      "           9     0.8140    0.5034    0.6220      2982\n",
      "\n",
      "    accuracy                         0.5539     30000\n",
      "   macro avg     0.6419    0.5539    0.5692     30000\n",
      "weighted avg     0.6422    0.5539    0.5693     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[4.6345e-05, 5.4424e-05, 4.5405e-05, 5.1008e-05],\n",
      "        [4.1405e-05, 5.3053e-05, 4.9137e-05, 4.8097e-05],\n",
      "        [4.4883e-05, 4.2449e-05, 3.7072e-05, 4.6662e-05],\n",
      "        [4.9917e-02, 4.6085e-02, 6.5808e-02, 4.2666e-02]])\n",
      "head_loss\n",
      " tensor([[0.9887, 1.0285, 0.9682, 1.0119],\n",
      "        [1.0372, 1.0077, 0.9875, 1.0001],\n",
      "        [1.0803, 1.0076, 0.9751, 1.0557],\n",
      "        [0.9583, 1.0273, 1.0379, 1.0154]])\n",
      "layer_loss\n",
      " tensor([1.0582, 1.0327, 0.9487, 1.1038])\n",
      "tensor([[1.0462, 1.0883, 1.0246, 1.0707],\n",
      "        [1.0712, 1.0407, 1.0198, 1.0329],\n",
      "        [1.0248, 0.9559, 0.9250, 1.0015],\n",
      "        [1.0578, 1.1339, 1.1456, 1.1207]])\n",
      "head_score\n",
      " tensor([[4.8486e-05, 5.9232e-05, 4.6520e-05, 5.4616e-05],\n",
      "        [4.4353e-05, 5.5210e-05, 5.0111e-05, 4.9677e-05],\n",
      "        [4.5998e-05, 4.0577e-05, 3.4293e-05, 4.6732e-05],\n",
      "        [5.2800e-02, 5.2257e-02, 7.5392e-02, 4.7818e-02]])\n",
      "[(2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[4.4678e-05, 5.5450e-05, 4.3826e-05, 4.9990e-05],\n",
      "        [3.8510e-05, 5.0121e-05, 4.7985e-05, 4.6656e-05],\n",
      "        [5.0306e-05, 0.0000e+00, 0.0000e+00, 5.1632e-05],\n",
      "        [5.7600e-02, 4.3736e-02, 6.2553e-02, 4.1175e-02]])\n",
      "head_loss\n",
      " tensor([[0.9680, 1.0019, 0.9448, 0.9872],\n",
      "        [1.0073, 0.9801, 0.9522, 0.9649],\n",
      "        [1.0674, 0.9793, 0.9793, 1.0342],\n",
      "        [0.9296, 0.9870, 0.9995, 0.9817]])\n",
      "layer_loss\n",
      " tensor([1.0303, 0.9890, 0.9341, 1.0484])\n",
      "tensor([[0.9973, 1.0322, 0.9734, 1.0172],\n",
      "        [0.9962, 0.9692, 0.9417, 0.9542],\n",
      "        [0.9971, 0.9148, 0.9148, 0.9661],\n",
      "        [0.9746, 1.0348, 1.0479, 1.0292]])\n",
      "head_score\n",
      " tensor([[4.4559e-05, 5.7237e-05, 4.2659e-05, 5.0848e-05],\n",
      "        [3.8364e-05, 4.8579e-05, 4.5188e-05, 4.4521e-05],\n",
      "        [5.0159e-05, 0.0000e+00, 0.0000e+00, 4.9879e-05],\n",
      "        [5.6139e-02, 4.5256e-02, 6.5551e-02, 4.2377e-02]])\n",
      "[(0, 2), (1, 0), (2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[4.8162e-05, 5.9203e-05, 0.0000e+00, 5.2435e-05],\n",
      "        [0.0000e+00, 5.3946e-05, 5.1208e-05, 5.1442e-05],\n",
      "        [4.8685e-05, 0.0000e+00, 0.0000e+00, 5.9866e-05],\n",
      "        [5.9935e-02, 4.3044e-02, 6.1431e-02, 4.4298e-02]])\n",
      "head_loss\n",
      " tensor([[0.9653, 0.9996, 0.9731, 0.9841],\n",
      "        [0.9731, 0.9740, 0.9556, 0.9593],\n",
      "        [1.0517, 0.9731, 0.9731, 1.0472],\n",
      "        [0.9249, 0.9735, 0.9856, 0.9719]])\n",
      "layer_loss\n",
      " tensor([0.9897, 0.9783, 0.9646, 1.0718])\n",
      "tensor([[0.9554, 0.9893, 0.9631, 0.9740],\n",
      "        [0.9520, 0.9529, 0.9349, 0.9385],\n",
      "        [1.0145, 0.9386, 0.9386, 1.0102],\n",
      "        [0.9914, 1.0434, 1.0564, 1.0417]])\n",
      "head_score\n",
      " tensor([[4.6013e-05, 5.8570e-05, 0.0000e+00, 5.1071e-05],\n",
      "        [0.0000e+00, 5.1404e-05, 4.7876e-05, 4.8281e-05],\n",
      "        [4.9389e-05, 0.0000e+00, 0.0000e+00, 6.0474e-05],\n",
      "        [5.9418e-02, 4.4914e-02, 6.4894e-02, 4.6148e-02]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 2), (2, 1), (2, 2)]\n",
      "head_importance_list\n",
      " tensor([[0.0000e+00, 7.2888e-05, 0.0000e+00, 6.1467e-05],\n",
      "        [0.0000e+00, 5.9134e-05, 0.0000e+00, 5.9818e-05],\n",
      "        [4.7942e-05, 0.0000e+00, 0.0000e+00, 7.2261e-05],\n",
      "        [6.0397e-02, 4.4577e-02, 5.6769e-02, 4.5858e-02]])\n",
      "head_loss\n",
      " tensor([[0.9581, 1.0119, 0.9581, 0.9874],\n",
      "        [0.9581, 0.9636, 0.9581, 0.9561],\n",
      "        [1.0195, 0.9581, 0.9581, 1.0524],\n",
      "        [0.9163, 0.9454, 0.9618, 0.9560]])\n",
      "layer_loss\n",
      " tensor([0.9708, 0.9854, 0.9618, 1.1568])\n",
      "tensor([[0.9301, 0.9823, 0.9301, 0.9586],\n",
      "        [0.9441, 0.9496, 0.9441, 0.9422],\n",
      "        [0.9805, 0.9215, 0.9215, 1.0122],\n",
      "        [1.0600, 1.0936, 1.1126, 1.1059]])\n",
      "head_score\n",
      " tensor([[0.0000e+00, 7.1599e-05, 0.0000e+00, 5.8921e-05],\n",
      "        [0.0000e+00, 5.6151e-05, 0.0000e+00, 5.6359e-05],\n",
      "        [4.7010e-05, 0.0000e+00, 0.0000e+00, 7.3147e-05],\n",
      "        [6.4019e-02, 4.8749e-02, 6.3160e-02, 5.0714e-02]])\n",
      "[(0, 0), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84744c2ea98944c9a9ef719db1dba697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3791\n",
      "Precision: 0.6435, Recall: 0.5654, F1-Score: 0.5801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3877    0.6273    0.4793      2992\n",
      "           1     0.6830    0.4141    0.5156      2992\n",
      "           2     0.6788    0.5843    0.6280      3012\n",
      "           3     0.2984    0.6681    0.4125      2998\n",
      "           4     0.7938    0.6680    0.7255      2973\n",
      "           5     0.9009    0.6785    0.7740      3054\n",
      "           6     0.6405    0.4099    0.4999      3003\n",
      "           7     0.5551    0.6421    0.5954      3012\n",
      "           8     0.6959    0.4467    0.5441      2982\n",
      "           9     0.8007    0.5148    0.6267      2982\n",
      "\n",
      "    accuracy                         0.5656     30000\n",
      "   macro avg     0.6435    0.5654    0.5801     30000\n",
      "weighted avg     0.6437    0.5656    0.5804     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[6.4833e-05, 3.1870e-05, 4.2578e-05, 4.4088e-05],\n",
      "        [2.3413e-05, 6.2936e-05, 3.9190e-05, 4.1970e-05],\n",
      "        [2.9118e-05, 3.0716e-05, 3.0342e-05, 3.0619e-05],\n",
      "        [5.8049e-02, 5.2407e-02, 3.7839e-02, 4.1493e-02]])\n",
      "head_loss\n",
      " tensor([[1.0131, 0.9129, 0.9529, 0.9967],\n",
      "        [0.9476, 0.9977, 0.9819, 0.9785],\n",
      "        [0.9447, 0.9619, 0.9707, 0.9722],\n",
      "        [1.0007, 0.9823, 0.9569, 0.9321]])\n",
      "layer_loss\n",
      " tensor([1.0801, 1.0454, 1.0753, 1.0700])\n",
      "tensor([[1.0943, 0.9860, 1.0293, 1.0766],\n",
      "        [0.9906, 1.0430, 1.0264, 1.0229],\n",
      "        [1.0159, 1.0343, 1.0438, 1.0455],\n",
      "        [1.0708, 1.0511, 1.0239, 0.9973]])\n",
      "head_score\n",
      " tensor([[7.0948e-05, 3.1425e-05, 4.3826e-05, 4.7466e-05],\n",
      "        [2.3192e-05, 6.5643e-05, 4.0226e-05, 4.2932e-05],\n",
      "        [2.9580e-05, 3.1770e-05, 3.1672e-05, 3.2011e-05],\n",
      "        [6.2160e-02, 5.5083e-02, 3.8741e-02, 4.1381e-02]])\n",
      "[(1, 0), (2, 0)]\n",
      "head_importance_list\n",
      " tensor([[6.4992e-05, 3.4432e-05, 4.4642e-05, 4.8280e-05],\n",
      "        [0.0000e+00, 6.6235e-05, 4.1120e-05, 4.4322e-05],\n",
      "        [0.0000e+00, 3.4053e-05, 3.5136e-05, 3.5456e-05],\n",
      "        [5.6580e-02, 4.9177e-02, 3.6148e-02, 4.1513e-02]])\n",
      "head_loss\n",
      " tensor([[1.0242, 0.9204, 0.9692, 1.0182],\n",
      "        [0.9476, 1.0112, 0.9905, 0.9939],\n",
      "        [0.9476, 0.9750, 0.9891, 0.9857],\n",
      "        [1.0038, 0.9834, 0.9604, 0.9389]])\n",
      "layer_loss\n",
      " tensor([1.0801, 1.0486, 1.0947, 1.1003])\n",
      "tensor([[1.1063, 0.9942, 1.0469, 1.0998],\n",
      "        [0.9937, 1.0603, 1.0387, 1.0422],\n",
      "        [1.0374, 1.0674, 1.0827, 1.0790],\n",
      "        [1.1044, 1.0820, 1.0566, 1.0330]])\n",
      "head_score\n",
      " tensor([[7.1901e-05, 3.4232e-05, 4.6736e-05, 5.3099e-05],\n",
      "        [0.0000e+00, 7.0230e-05, 4.2710e-05, 4.6193e-05],\n",
      "        [0.0000e+00, 3.6348e-05, 3.8042e-05, 3.8258e-05],\n",
      "        [6.2488e-02, 5.3208e-02, 3.8196e-02, 4.2883e-02]])\n",
      "[(0, 1), (1, 0), (2, 0), (2, 1)]\n",
      "head_importance_list\n",
      " tensor([[6.8252e-05, 0.0000e+00, 4.7232e-05, 5.0154e-05],\n",
      "        [0.0000e+00, 6.8980e-05, 4.1745e-05, 4.5273e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 4.1910e-05, 4.0699e-05],\n",
      "        [5.4009e-02, 4.7766e-02, 3.5461e-02, 4.2682e-02]])\n",
      "head_loss\n",
      " tensor([[1.0196, 0.9412, 0.9674, 1.0076],\n",
      "        [0.9412, 1.0212, 0.9847, 0.9955],\n",
      "        [0.9412, 0.9412, 0.9954, 0.9825],\n",
      "        [0.9875, 0.9671, 0.9474, 0.9258]])\n",
      "layer_loss\n",
      " tensor([1.0631, 1.0486, 1.1051, 1.0939])\n",
      "tensor([[1.0840, 1.0006, 1.0285, 1.0712],\n",
      "        [0.9869, 1.0709, 1.0325, 1.0439],\n",
      "        [1.0401, 1.0401, 1.1000, 1.0857],\n",
      "        [1.0802, 1.0579, 1.0364, 1.0127]])\n",
      "head_score\n",
      " tensor([[7.3984e-05, 0.0000e+00, 4.8577e-05, 5.3727e-05],\n",
      "        [0.0000e+00, 7.3867e-05, 4.3102e-05, 4.7260e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 4.6102e-05, 4.4188e-05],\n",
      "        [5.8340e-02, 5.0530e-02, 3.6752e-02, 4.3222e-02]])\n",
      "[(0, 1), (1, 0), (1, 2), (2, 0), (2, 1), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[7.5664e-05, 0.0000e+00, 5.3484e-05, 5.6438e-05],\n",
      "        [0.0000e+00, 8.2749e-05, 0.0000e+00, 5.7767e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 5.5814e-05, 0.0000e+00],\n",
      "        [5.5908e-02, 4.8733e-02, 4.0829e-02, 4.4618e-02]])\n",
      "head_loss\n",
      " tensor([[1.1280, 1.0326, 1.0710, 1.0990],\n",
      "        [1.0326, 1.1448, 1.0326, 1.1150],\n",
      "        [1.0326, 1.0326, 1.1149, 1.0326],\n",
      "        [1.0667, 1.0525, 1.0275, 1.0075]])\n",
      "layer_loss\n",
      " tensor([1.1567, 1.1667, 1.1580, 1.1765])\n",
      "tensor([[1.3048, 1.1944, 1.2388, 1.2713],\n",
      "        [1.2046, 1.3356, 1.2046, 1.3008],\n",
      "        [1.1957, 1.1957, 1.2911, 1.1957],\n",
      "        [1.2551, 1.2383, 1.2089, 1.1854]])\n",
      "head_score\n",
      " tensor([[9.8729e-05, 0.0000e+00, 6.6258e-05, 7.1749e-05],\n",
      "        [0.0000e+00, 1.1052e-04, 0.0000e+00, 7.5143e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 7.2059e-05, 0.0000e+00],\n",
      "        [7.0168e-02, 6.0345e-02, 4.9359e-02, 5.2891e-02]])\n",
      "[(0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (2, 0), (2, 1), (2, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7314f88f8900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/jieungkim/anaconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7314f88f8900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jieungkim/.cache/pypoetry/virtualenvs/decomposetransformer-UESb9BbT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/jieungkim/anaconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4ed5e97e6e40799167de23906e0665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4532\n",
      "Precision: 0.6171, Recall: 0.5445, F1-Score: 0.5556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3958    0.6243    0.4845      2992\n",
      "           1     0.5960    0.4733    0.5276      2992\n",
      "           2     0.6641    0.6066    0.6340      3012\n",
      "           3     0.3000    0.6391    0.4083      2998\n",
      "           4     0.8094    0.5658    0.6660      2973\n",
      "           5     0.8992    0.3854    0.5395      3054\n",
      "           6     0.6285    0.3989    0.4881      3003\n",
      "           7     0.5098    0.5156    0.5127      3012\n",
      "           8     0.6368    0.6402    0.6385      2982\n",
      "           9     0.7309    0.5956    0.6563      2982\n",
      "\n",
      "    accuracy                         0.5441     30000\n",
      "   macro avg     0.6171    0.5445    0.5556     30000\n",
      "weighted avg     0.6174    0.5441    0.5554     30000\n",
      "\n",
      "Total heads to prune: 8\n",
      "head_importance_list\n",
      " tensor([[5.7599e-05, 4.4358e-05, 4.0555e-05, 4.2634e-05],\n",
      "        [4.4702e-05, 4.6824e-05, 4.9800e-05, 4.5123e-05],\n",
      "        [3.1682e-05, 5.0094e-05, 3.6170e-05, 2.8265e-05],\n",
      "        [5.0960e-02, 6.5365e-02, 7.2209e-02, 6.9980e-02]])\n",
      "head_loss\n",
      " tensor([[1.2101, 1.1216, 1.1509, 1.1329],\n",
      "        [1.1677, 1.1085, 1.1627, 1.1433],\n",
      "        [1.1307, 1.1169, 1.1615, 1.1164],\n",
      "        [1.1172, 1.0895, 1.0888, 1.1228]])\n",
      "layer_loss\n",
      " tensor([1.2058, 1.0136, 1.1255, 1.0941])\n",
      "tensor([[1.4591, 1.3525, 1.3878, 1.3660],\n",
      "        [1.1836, 1.1235, 1.1784, 1.1589],\n",
      "        [1.2726, 1.2571, 1.3073, 1.2566],\n",
      "        [1.2223, 1.1920, 1.1913, 1.2285]])\n",
      "head_score\n",
      " tensor([[8.4046e-05, 5.9992e-05, 5.6281e-05, 5.8239e-05],\n",
      "        [5.2908e-05, 5.2609e-05, 5.8686e-05, 5.2291e-05],\n",
      "        [4.0319e-05, 6.2972e-05, 4.7286e-05, 3.5517e-05],\n",
      "        [6.2289e-02, 7.7917e-02, 8.6022e-02, 8.5969e-02]])\n",
      "[(2, 0), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[5.5130e-05, 4.6912e-05, 3.9775e-05, 4.5845e-05],\n",
      "        [5.2154e-05, 4.7800e-05, 5.3591e-05, 4.9542e-05],\n",
      "        [0.0000e+00, 5.7229e-05, 4.0836e-05, 0.0000e+00],\n",
      "        [5.6590e-02, 6.9635e-02, 7.2883e-02, 7.2089e-02]])\n",
      "head_loss\n",
      " tensor([[1.1628, 1.0889, 1.1105, 1.1017],\n",
      "        [1.1549, 1.0845, 1.1496, 1.1243],\n",
      "        [1.1032, 1.0682, 1.1220, 1.1032],\n",
      "        [1.0734, 1.0649, 1.0467, 1.0752]])\n",
      "layer_loss\n",
      " tensor([1.1812, 1.0149, 1.1084, 1.0920])\n",
      "tensor([[1.3735, 1.2862, 1.3117, 1.3013],\n",
      "        [1.1722, 1.1007, 1.1668, 1.1411],\n",
      "        [1.2227, 1.1840, 1.2436, 1.2227],\n",
      "        [1.1721, 1.1629, 1.1430, 1.1741]])\n",
      "head_score\n",
      " tensor([[7.5719e-05, 6.0339e-05, 5.2172e-05, 5.9659e-05],\n",
      "        [6.1133e-05, 5.2612e-05, 6.2530e-05, 5.6531e-05],\n",
      "        [0.0000e+00, 6.7757e-05, 5.0783e-05, 0.0000e+00],\n",
      "        [6.6330e-02, 8.0977e-02, 8.3304e-02, 8.4641e-02]])\n",
      "[(0, 2), (2, 0), (2, 2), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[5.8812e-05, 4.9629e-05, 0.0000e+00, 5.1336e-05],\n",
      "        [5.5611e-05, 4.6243e-05, 5.6067e-05, 5.4221e-05],\n",
      "        [0.0000e+00, 5.9709e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [6.4565e-02, 8.3003e-02, 7.5556e-02, 7.6320e-02]])\n",
      "head_loss\n",
      " tensor([[1.1910, 1.1078, 1.1243, 1.1227],\n",
      "        [1.1887, 1.1117, 1.1754, 1.1628],\n",
      "        [1.1243, 1.0860, 1.1243, 1.1243],\n",
      "        [1.0878, 1.1010, 1.0614, 1.0912]])\n",
      "layer_loss\n",
      " tensor([1.2191, 1.0685, 1.1084, 1.1298])\n",
      "tensor([[1.4520, 1.3506, 1.3707, 1.3687],\n",
      "        [1.2701, 1.1879, 1.2559, 1.2425],\n",
      "        [1.2461, 1.2037, 1.2461, 1.2461],\n",
      "        [1.2289, 1.2439, 1.1992, 1.2328]])\n",
      "head_score\n",
      " tensor([[8.5397e-05, 6.7028e-05, 0.0000e+00, 7.0262e-05],\n",
      "        [7.0632e-05, 5.4931e-05, 7.0417e-05, 6.7370e-05],\n",
      "        [0.0000e+00, 7.1873e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [7.9345e-02, 1.0325e-01, 9.0606e-02, 9.4088e-02]])\n",
      "[(0, 1), (0, 2), (1, 1), (2, 0), (2, 2), (2, 3)]\n",
      "head_importance_list\n",
      " tensor([[6.2088e-05, 0.0000e+00, 0.0000e+00, 5.7175e-05],\n",
      "        [6.6374e-05, 0.0000e+00, 7.0272e-05, 6.4301e-05],\n",
      "        [0.0000e+00, 5.2062e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [7.0698e-02, 9.8649e-02, 7.3585e-02, 7.8287e-02]])\n",
      "head_loss\n",
      " tensor([[1.1743, 1.1021, 1.1021, 1.1076],\n",
      "        [1.1858, 1.1021, 1.1806, 1.1648],\n",
      "        [1.1021, 1.0776, 1.1021, 1.1021],\n",
      "        [1.0624, 1.0943, 1.0374, 1.0680]])\n",
      "layer_loss\n",
      " tensor([1.2179, 1.0685, 1.1096, 1.1430])\n",
      "tensor([[1.4302, 1.3422, 1.3422, 1.3489],\n",
      "        [1.2671, 1.1776, 1.2614, 1.2447],\n",
      "        [1.2229, 1.1957, 1.2229, 1.2229],\n",
      "        [1.2143, 1.2507, 1.1857, 1.2206]])\n",
      "head_score\n",
      " tensor([[8.8797e-05, 0.0000e+00, 0.0000e+00, 7.7122e-05],\n",
      "        [8.4099e-05, 0.0000e+00, 8.8644e-05, 8.0032e-05],\n",
      "        [0.0000e+00, 6.2252e-05, 0.0000e+00, 0.0000e+00],\n",
      "        [8.5847e-02, 1.2338e-01, 8.7248e-02, 9.5560e-02]])\n",
      "[(0, 1), (0, 2), (0, 3), (1, 1), (2, 0), (2, 1), (2, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc944b71cef4a57a13986a6cfc26d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the model:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3779\n",
      "Precision: 0.6321, Recall: 0.5727, F1-Score: 0.5846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4045    0.6126    0.4873      2992\n",
      "           1     0.6143    0.4499    0.5194      2992\n",
      "           2     0.7165    0.5378    0.6145      3012\n",
      "           3     0.3102    0.6481    0.4196      2998\n",
      "           4     0.8340    0.5291    0.6475      2973\n",
      "           5     0.8261    0.7449    0.7834      3054\n",
      "           6     0.6487    0.3966    0.4923      3003\n",
      "           7     0.5771    0.5631    0.5700      3012\n",
      "           8     0.6858    0.5879    0.6331      2982\n",
      "           9     0.7036    0.6566    0.6793      2982\n",
      "\n",
      "    accuracy                         0.5729     30000\n",
      "   macro avg     0.6321    0.5727    0.5846     30000\n",
      "weighted avg     0.6323    0.5729    0.5849     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "\n",
    "for concern in range(config.num_labels):\n",
    "  config.init_seed()\n",
    "  positive_samples = SamplingDataset(\n",
    "      train_dataloader,\n",
    "      config,\n",
    "      concern,\n",
    "      num_samples,\n",
    "      True,\n",
    "      4,\n",
    "      resample=False,\n",
    "  )\n",
    "  negative_samples = SamplingDataset(\n",
    "      train_dataloader,\n",
    "      config,\n",
    "      concern,\n",
    "      num_samples,\n",
    "      False,\n",
    "      4,\n",
    "      resample=False,\n",
    "  )\n",
    "  module = copy.deepcopy(model)\n",
    "\n",
    "  head_importance_prunning(module, config, positive_samples, ratio)\n",
    "\n",
    "#   print(f\"Evaluate the pruned model {concern}\")\n",
    "  result = evaluate_model(module, config, test_dataloader)\n",
    "#   result_list.append(result)\n",
    "#   get_similarity(model, module, valid_dataloader, concern, num_samples, config)\n",
    "#   print(\"original model's perplexity\")\n",
    "#   get_perplexity(model, valid_dataloader, config)\n",
    "#   print(\"pruned model's perplexity\")\n",
    "#   get_perplexity(module, valid_dataloader, config)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decomposetransformer-UESb9BbT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
